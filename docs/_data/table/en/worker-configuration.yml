alluxio.worker.allocator.class:
  'The strategy that a worker uses to allocate space among storage directories in certain storage layer. Valid options include: `alluxio.worker.block.allocator.MaxFreeAllocator`, `alluxio.worker.block.allocator.GreedyAllocator`, `alluxio.worker.block.allocator.RoundRobinAllocator`.'
alluxio.worker.bind.host:
  'The hostname Alluxio''s worker node binds to.'
alluxio.worker.block.annotator.class:
  'The strategy that a worker uses to annotate blocks in order to have an ordered view of them during internalmanagement tasks such as eviction and promotion/demotion.  Valid options include: `alluxio.worker.block.annotator.LRFUAnnotator`, `alluxio.worker.block.annotator.LRUAnnotator`, '
alluxio.worker.block.annotator.lrfu.attenuation.factor:
  'A attenuation factor in [2, INF) to control the behavior of LRFU annotator.'
alluxio.worker.block.annotator.lrfu.step.factor:
  'A factor in [0, 1] to control the behavior of LRFU: smaller value makes LRFU more similar to LFU; and larger value makes LRFU closer to LRU.'
alluxio.worker.block.heartbeat.interval:
  'The interval between block workers'' heartbeats to update block status, storage health and other workers'' information to Alluxio Master.'
alluxio.worker.block.heartbeat.report.size.threshold:
  'When alluxio.worker.register.to.all.masters=true, because a worker will send block reports to all masters, we use a threshold to limit the unsent block report size in worker''s memory. If the worker block heartbeat is larger than the threshold, we discard the heartbeat message and force the worker to register with that master with a full report.'
alluxio.worker.block.heartbeat.timeout:
  'The timeout value of block workers'' heartbeats. If the worker can''t connect to master before this interval expires, the worker will exit.'
alluxio.worker.block.master.client.pool.size:
  'The block master client pool size on the Alluxio workers.'
alluxio.worker.block.store.type:
  'The implementation of LocalBlockStore that can be instantiated.'
alluxio.worker.container.hostname:
  'The container hostname if worker is running in a container.'
alluxio.worker.data.bind.host:
  'The hostname that the Alluxio worker''s data server runs on. See &lt;a href=&quot;#configure-multihomed-networks&quot;&gt;multi-homed networks&lt;/a&gt;.'
alluxio.worker.data.folder:
  'A relative path within each storage directory used as the data folder for Alluxio worker to put data for tiered store.'
alluxio.worker.data.folder.permissions:
  'The permission set for the worker data folder. If short circuit is used this folder should be accessible by all users (rwxrwxrwx).'
alluxio.worker.data.folder.tmp:
  'A relative path in alluxio.worker.data.folder used to store the temporary data for uncommitted files.'
alluxio.worker.data.hostname:
  'The hostname of Alluxio worker data service.'
alluxio.worker.data.port:
  'The port Alluxio''s worker''s data server runs on.'
alluxio.worker.data.server.class:
  'Selects the networking stack to run the worker with. Valid options are: `alluxio.worker.netty.NettyDataServer`.'
alluxio.worker.data.server.domain.socket.address:
  'The path to the domain socket. Short-circuit reads make use of a UNIX domain socket when this is set (non-empty). This is a special path in the file system that allows the client and the AlluxioWorker to communicate. You will need to set a path to this socket. The AlluxioWorker needs to be able to create the path. If alluxio.worker.data.server.domain.socket.as.uuid is set, the path should be the home directory for the domain socket. The full path for the domain socket with be {path}/{uuid}.'
alluxio.worker.data.server.domain.socket.as.uuid:
  'If true, the property alluxio.worker.data.server.domain.socket.addressis the path to the home directory for the domain socket and a unique identifier is used as the domain socket name. If false, the property is the absolute path to the UNIX domain socket.'
alluxio.worker.data.tmp.subdir.max:
  'The maximum number of sub-directories allowed to be created in ${alluxio.worker.data.tmp.folder}.'
alluxio.worker.evictor.class:
  'The strategy that a worker uses to evict block files when a storage layer runs out of space. Valid options include `alluxio.worker.block.evictor.LRFUEvictor`, `alluxio.worker.block.evictor.GreedyEvictor`, `alluxio.worker.block.evictor.LRUEvictor`, `alluxio.worker.block.evictor.PartialLRUEvictor`.'
alluxio.worker.file.buffer.size:
  'The buffer size for worker to write data into the tiered storage.'
alluxio.worker.free.space.timeout:
  'The duration for which a worker will wait for eviction to make space available for a client write request.'
alluxio.worker.fuse.enabled:
  'If true, launch worker embedded Fuse application.'
alluxio.worker.hostname:
  'The hostname of Alluxio worker.'
alluxio.worker.jvm.monitor.enabled:
  'Whether to enable start JVM monitor thread on the worker. This will start a thread to detect JVM-wide pauses induced by GC or other reasons.'
alluxio.worker.keytab.file:
  'Kerberos keytab file for Alluxio worker.'
alluxio.worker.management.backoff.strategy:
  'Defines the backoff scope respected by background tasks. Supported values are ANY / DIRECTORY. ANY: Management tasks will backoff from worker when there is any user I/O.This mode will ensure low management task overhead in order to favor immediate user I/O performance. However, making progress on management tasks will require quite periods on the worker.DIRECTORY: Management tasks will backoff from directories with ongoing user I/O.This mode will give better chance of making progress on management tasks.However, immediate user I/O throughput might be reduced due to increased management task activity.'
alluxio.worker.management.block.transfer.concurrency.limit:
  'Puts a limit to how many block transfers are executed concurrently during management.'
alluxio.worker.management.load.detection.cool.down.time:
  'Management tasks will not run for this long after load detected. Any user I/O will still register as a load for this period of time after it is finished. Short durations might cause interference between user I/O and background tier management tasks. Long durations might cause starvation for background tasks.'
alluxio.worker.management.task.thread.count:
  'The number of threads for management task executor'
alluxio.worker.management.tier.align.enabled:
  'Whether to align tiers based on access pattern.'
alluxio.worker.management.tier.align.range:
  'Maximum number of blocks to consider from one tier for a single alignment task.'
alluxio.worker.management.tier.align.reserved.bytes:
  'The amount of space that is reserved from each storage directory for internal management tasks.'
alluxio.worker.management.tier.promote.enabled:
  'Whether to promote blocks to higher tiers.'
alluxio.worker.management.tier.promote.quota.percent:
  'Max percentage of each tier that could be used for promotions. Promotions will be stopped to a tier once its used space go over this value. (0 means never promote, and, 100 means always promote.'
alluxio.worker.management.tier.promote.range:
  'Maximum number of blocks to consider from one tier for a single promote task.'
alluxio.worker.management.tier.swap.restore.enabled:
  'Whether to run management swap-restore task when tier alignment cannot make progress.'
alluxio.worker.master.connect.retry.timeout:
  'Retry period before workers give up on connecting to master and exit.'
alluxio.worker.master.periodical.rpc.timeout:
  'Timeout for periodical RPC between workers and the leading master. This property is added to prevent workers from hanging in periodical RPCs with previous leading master during flaky network situations. If the timeout is too short, periodical RPCs may not have enough time to get response from the leading master during heavy cluster load and high network latency.'
alluxio.worker.network.async.cache.manager.queue.max:
  'The maximum number of outstanding async caching requests to cache blocks in each data server'
alluxio.worker.network.async.cache.manager.threads.max:
  'The maximum number of threads used to cache blocks asynchronously in the data server.'
alluxio.worker.network.block.reader.threads.max:
  'The maximum number of threads used to read blocks in the data server.'
alluxio.worker.network.block.writer.threads.max:
  'The maximum number of threads used to write blocks in the data server.'
alluxio.worker.network.flowcontrol.window:
  'The HTTP2 flow control window used by worker gRPC connections. Larger value will allow more data to be buffered but will use more memory.'
alluxio.worker.network.keepalive.time:
  'The amount of time for data server (for block reads and block writes) to wait for a response before pinging the client to see if it is still alive.'
alluxio.worker.network.keepalive.timeout:
  'The maximum time for a data server (for block reads and block writes) to wait for a keepalive response before closing the connection.'
alluxio.worker.network.max.inbound.message.size:
  'The max inbound message size used by worker gRPC connections.'
alluxio.worker.network.netty.async.cache.manager.threads.max:
  'The maximum number of threads used to cache blocks asynchronously in the netty data server.'
alluxio.worker.network.netty.backlog:
  'Netty socket option for SO_BACKLOG: the number of connections queued.'
alluxio.worker.network.netty.block.reader.threads.max:
  'The maximum number of threads used to read blocks in the netty data server.'
alluxio.worker.network.netty.block.writer.threads.max:
  'The maximum number of threads used to write blocks in the netty data server.'
alluxio.worker.network.netty.boss.threads:
  'How many threads to use for accepting new requests.'
alluxio.worker.network.netty.buffer.receive:
  'Netty socket option for SO_RCVBUF: the proposed buffer size that will be used for receives.'
alluxio.worker.network.netty.buffer.send:
  'Netty socket option for SO_SNDBUF: the proposed buffer size that will be used for sends.'
alluxio.worker.network.netty.channel:
  'Netty channel type: NIO or EPOLL. If EPOLL is not available, this will automatically fall back to NIO.'
alluxio.worker.network.netty.file.transfer:
  'When returning files to the user, select how the data is transferred; valid options are `MAPPED` (uses java MappedByteBuffer) and `TRANSFER` (uses Java FileChannel.transferTo).'
alluxio.worker.network.netty.file.writer.threads.max:
  'The maximum number of threads used to write files to UFS in the netty data server.'
alluxio.worker.network.netty.reader.buffer.size.packets:
  'The maximum number of parallel data packets when a client reads from a worker.'
alluxio.worker.network.netty.rpc.threads.max:
  'The maximum number of threads used to handle worker side RPCs in the netty data server.'
alluxio.worker.network.netty.shutdown.quiet.period:
  'The quiet period. When the netty server is shutting down, it will ensure that no RPCs occur during the quiet period. If an RPC occurs, then the quiet period will restart before shutting down the netty server.'
alluxio.worker.network.netty.shutdown.timeout:
  'Maximum amount of time to wait until the netty server is shutdown (regardless of the quiet period).'
alluxio.worker.network.netty.watermark.high:
  'Determines how many bytes can be in the write queue before switching to non-writable.'
alluxio.worker.network.netty.watermark.low:
  'Once the high watermark limit is reached, the queue must be flushed down to the low watermark before switching back to writable.'
alluxio.worker.network.netty.worker.threads:
  'Number of threads to use for processing requests in worker'
alluxio.worker.network.netty.writer.buffer.size.packets:
  'The maximum number of parallel data packets when a client writes to a worker.'
alluxio.worker.network.permit.keepalive.time:
  'Specify the most aggressive keep-alive time clients are permitted to configure. The server will try to detect clients exceeding this rate and when detected will forcefully close the connection.'
alluxio.worker.network.reader.buffer.pooled:
  'Whether it is using pooled direct buffer or unpooled wrapped buffer when creating a buffer for remote read'
alluxio.worker.network.reader.buffer.size:
  'When a client reads from a remote worker, the maximum amount of data not received by client allowed before the worker pauses sending more data. If this value is lower than read chunk size, read performance may be impacted as worker waits more often for buffer to free up. Higher value will increase the memory consumed by each read request.'
alluxio.worker.network.reader.max.chunk.size.bytes:
  'When a client read from a remote worker, the maximum chunk size.'
alluxio.worker.network.shutdown.timeout:
  'Maximum amount of time to wait until the worker gRPC server is shutdown (regardless of the quiet period).'
alluxio.worker.network.writer.buffer.size.messages:
  'When a client writes to a remote worker, the maximum number of data messages to buffer by the server for each request.'
alluxio.worker.network.zerocopy.enabled:
  'Whether zero copy is enabled on worker when processing data streams.'
alluxio.worker.page.store.async.restore.enabled:
  'If this is enabled, cache restore state asynchronously.'
alluxio.worker.page.store.async.write.enabled:
  'If this is enabled, cache data asynchronously.'
alluxio.worker.page.store.async.write.threads:
  'Number of threads to asynchronously cache data.'
alluxio.worker.page.store.dirs:
  'A list of the directories where pages in paged block store are stored.'
alluxio.worker.page.store.eviction.retries:
  'Max number of eviction retries.'
alluxio.worker.page.store.evictor.class:
  'The strategy that worker uses to evict local cached pages when running out of space. Currently valid options include `alluxio.client.file.cache.evictor.LRUCacheEvictor`,`alluxio.client.file.cache.evictor.LFUCacheEvictor`.'
alluxio.worker.page.store.evictor.lfu.logbase:
  'The log base for client cache LFU evictor bucket index.'
alluxio.worker.page.store.evictor.nondeterministic.enabled:
  'If this is enabled, the evictor picks uniformly from the worst k elements.Currently only LRU is supported.'
alluxio.worker.page.store.local.store.file.buckets:
  'The number of file buckets for the page blocked store on local file system. It is recommended to set this to a high value if the number of unique files is expected to be high (# files / file buckets &lt;= 100,000).'
alluxio.worker.page.store.overhead:
  'A fraction value representing the storage overhead writing to disk. For example, with 1GB allocated cache space, and 10% storage overhead we expect no more than 1024MB / (1 + 10%) user data to store.'
alluxio.worker.page.store.page.size:
  'Size of each page in worker paged block store.'
alluxio.worker.page.store.quota.enabled:
  'Whether to support cache quota.'
alluxio.worker.page.store.sizes:
  'A list of maximum cache size for each cache directory.'
alluxio.worker.page.store.timeout.duration:
  'The timeout duration for local cache I/O operations (reading/writing/deleting). When this property is a positive value,local cache operations after timing out will fail and fallback to external file system but transparent to applications; when this property is a negative value, this feature is disabled.'
alluxio.worker.page.store.timeout.threads:
  'The number of threads to handle cache I/O operation timeout, when alluxio.worker.page.store.timeout.duration is positive.'
alluxio.worker.page.store.type:
  'The type of page store to use for worker page store. Can be either `LOCAL` or `ROCKS`. The `LOCAL` page store stores all pages in a directory, the `ROCKS` page store utilizes rocksDB to persist the data.'
alluxio.worker.principal:
  'Kerberos principal for Alluxio worker.'
alluxio.worker.ramdisk.size:
  'The allocated memory for each worker node''s ramdisk(s). It is recommended to set this value explicitly.'
alluxio.worker.register.lease.enabled:
  'Whether the worker requests a lease from the master before registering.This should be consistent with alluxio.master.worker.register.lease.enabled'
alluxio.worker.register.lease.retry.max.duration:
  'The total time on retrying to get a register lease, before giving up.'
alluxio.worker.register.lease.retry.sleep.max:
  'The maximum time to sleep before retrying to get a register lease.'
alluxio.worker.register.lease.retry.sleep.min:
  'The minimum time to sleep before retrying to get a register lease.'
alluxio.worker.register.stream.batch.size:
  'When the worker registers with the master using a stream, this defines the metadata of how many blocks should be send to the master in each batch.'
alluxio.worker.register.stream.complete.timeout:
  'When the worker registers the master with streaming, after all messages have been sent to the master, the worker will wait for the registration to complete on the master side. If the master is unable to finish the registration and return success to the worker within this timeout, the worker will consider the registration failed.'
alluxio.worker.register.stream.deadline:
  'When the worker registers with the master using a stream, this defines the total deadline for the full stream to finish.'
alluxio.worker.register.stream.enabled:
  'When the worker registers with the master, whether the request should be broken into a stream of smaller batches. This is useful when the worker''s storage is large and we expect a large number of blocks. '
alluxio.worker.register.stream.response.timeout:
  'When the worker registers the master with streaming, the worker will be sending messages to the master during the streaming.During an active stream if the master have not responded to the worker for more than this timeout, the worker will consider the master is hanging and close the stream.'
alluxio.worker.register.to.all.masters:
  'If enabled, workers will register themselves to all masters, instead of primary master only. This can be used to save the master failover time because the new primary immediately knows all existing workers and blocks. Can only be enabled when alluxio.standby.master.grpc.enabled is turned on.'
alluxio.worker.remote.io.slow.threshold:
  'The time threshold for when a worker remote IO (read or write) of a single buffer is considered slow. When slow IO occurs, it is logged by a sampling logger.'
alluxio.worker.reviewer.class:
  '(Experimental) The API is subject to change in the future.The strategy that a worker uses to review space allocation in the Allocator. Each time a block allocation decision is made by the Allocator, the Reviewer will review the decision and rejects it,if the allocation does not meet certain criteria of the Reviewer.The Reviewer prevents the worker to make a bad block allocation decision.Valid options include:`alluxio.worker.block.reviewer.ProbabilisticBufferReviewer`.'
alluxio.worker.reviewer.probabilistic.hardlimit.bytes:
  'This is used by the `alluxio.worker.block.reviewer.ProbabilisticBufferReviewer`. When the free space in a storage dir falls below this hard limit, the ProbabilisticBufferReviewer will stop accepting new blocks into it.This is because we may load more data into existing blocks in the directory and their sizes may expand.'
alluxio.worker.reviewer.probabilistic.softlimit.bytes:
  'This is used by the `alluxio.worker.block.reviewer.ProbabilisticBufferReviewer`. We attempt to leave a buffer in each storage directory. When the free space in a certain storage directory on the worker falls below this soft limit, the chance that the Reviewer accepts new blocks into this directory goes down. This chance keeps falling linearly until it reaches 0, when the available space reaches the hard limit.'
alluxio.worker.rpc.executor.core.pool.size:
  'The number of threads to keep in thread pool of worker RPC ExecutorService.'
alluxio.worker.rpc.executor.fjp.async:
  'This property is effective when alluxio.worker.rpc.executor.type is set to ForkJoinPool. if true, it establishes local first-in-first-out scheduling mode for forked tasks that are never joined. This mode may be more appropriate than default locally stack-based mode in applications in which worker threads only process event-style asynchronous tasks.'
alluxio.worker.rpc.executor.fjp.min.runnable:
  'This property is effective when alluxio.worker.rpc.executor.type is set to ForkJoinPool. It controls the minimum allowed number of core threads not blocked. A value of 1 ensures liveness. A larger value might improve throughput but might also increase overhead.'
alluxio.worker.rpc.executor.fjp.parallelism:
  'This property is effective when alluxio.worker.rpc.executor.type is set to ForkJoinPool. It controls the parallelism level (internal queue count) of master RPC ExecutorService.'
alluxio.worker.rpc.executor.keepalive:
  'The keep alive time of a thread in worker RPC ExecutorServicelast used before this thread is terminated (and replaced if necessary).'
alluxio.worker.rpc.executor.max.pool.size:
  'The maximum number of threads allowed for worker RPC ExecutorService. When the maximum is reached, attempts to replace blocked threads fail.'
alluxio.worker.rpc.executor.tpe.allow.core.threads.timeout:
  'This property is effective when alluxio.worker.rpc.executor.type is set to ThreadPoolExecutor. It controls whether core threads can timeout and terminate when there is no work.'
alluxio.worker.rpc.executor.tpe.queue.type:
  'This property is effective when alluxio.worker.rpc.executor.type is set to TPE. It specifies the internal task queue that''s used by RPC ExecutorService. Supported values are: LINKED_BLOCKING_QUEUE, LINKED_BLOCKING_QUEUE_WITH_CAP, ARRAY_BLOCKING_QUEUE and SYNCHRONOUS_BLOCKING_QUEUE'
alluxio.worker.rpc.executor.type:
  'Type of ExecutorService for Alluxio worker gRPC server. Supported values are TPE (for ThreadPoolExecutor) and FJP (for ForkJoinPool).'
alluxio.worker.rpc.port:
  'The port for Alluxio worker''s RPC service.'
alluxio.worker.session.timeout:
  'Timeout between worker and client connection indicating a lost session connection.'
alluxio.worker.startup.timeout:
  'Maximum time to wait for worker startup.'
alluxio.worker.storage.checker.enabled:
  'Whether periodic storage health checker is enabled on Alluxio workers.'
alluxio.worker.tieredstore.block.lock.readers:
  'The max number of concurrent readers for a block lock.'
alluxio.worker.tieredstore.block.locks:
  'Total number of block locks for an Alluxio block worker. Larger value leads to finer locking granularity, but uses more space.'
alluxio.worker.tieredstore.free.ahead.bytes:
  'Amount to free ahead when worker storage is full. Higher values will help decrease CPU utilization under peak storage. Lower values will increase storage utilization.'
alluxio.worker.tieredstore.level0.alias:
  'The alias of the top storage tier on this worker. It must match one of the global storage tiers from the master configuration. We disable placing an alias lower in the global hierarchy before an alias with a higher position on the worker hierarchy. So by default, SSD cannot come before MEM on any worker.'
alluxio.worker.tieredstore.level0.dirs.mediumtype:
  'A comma-separated list of media types (e.g., &quot;MEM,MEM,SSD&quot;) for each storage directory on the top storage tier specified by alluxio.worker.tieredstore.level0.dirs.path.'
alluxio.worker.tieredstore.level0.dirs.path:
  'A comma-separated list of paths (eg., /mnt/ramdisk1,/mnt/ramdisk2,/mnt/ssd/alluxio/cache1) of storage directories for the top storage tier. Note that for MacOS, the root directory should be `/Volumes/` and not `/mnt/`.'
alluxio.worker.tieredstore.level0.dirs.quota:
  'A comma-separated list of capacities (e.g., &quot;500MB,500MB,5GB&quot;) for each storage directory on the top storage tier specified by alluxio.worker.tieredstore.level0.dirs.path. For any &quot;MEM&quot;-type media (i.e, the ramdisks), this value should be set equivalent to the value specified by alluxio.worker.ramdisk.size.'
alluxio.worker.tieredstore.level0.watermark.high.ratio:
  'The high watermark of the space in the top storage tier (a value between 0 and 1).'
alluxio.worker.tieredstore.level0.watermark.low.ratio:
  'The low watermark of the space in the top storage tier (a value between 0 and 1).'
alluxio.worker.tieredstore.level1.alias:
  'The alias of the second storage tier on this worker.'
alluxio.worker.tieredstore.level1.dirs.mediumtype:
  'A list of media types (e.g., &quot;SSD,SSD,HDD&quot;) for each storage directory on the second storage tier specified by alluxio.worker.tieredstore.level1.dirs.path.'
alluxio.worker.tieredstore.level1.dirs.path:
  'A comma-separated list of paths (eg., /mnt/ssd/alluxio/cache2,/mnt/ssd/alluxio/cache3,/mnt/hdd/alluxio/cache1) of storage directories for the second storage tier.'
alluxio.worker.tieredstore.level1.dirs.quota:
  'A comma-separated list of capacities (e.g., &quot;5GB,5GB,50GB&quot;) for each storage directory on the second storage tier specified by alluxio.worker.tieredstore.level1.dirs.path.'
alluxio.worker.tieredstore.level1.watermark.high.ratio:
  'The high watermark of the space in the second storage tier (a value between 0 and 1).'
alluxio.worker.tieredstore.level1.watermark.low.ratio:
  'The low watermark of the space in the second storage tier (a value between 0 and 1).'
alluxio.worker.tieredstore.level2.alias:
  'The alias of the third storage tier on this worker.'
alluxio.worker.tieredstore.level2.dirs.mediumtype:
  'A list of media types (e.g., &quot;SSD,HDD,HDD&quot;) for each storage directory on the third storage tier specified by alluxio.worker.tieredstore.level2.dirs.path.'
alluxio.worker.tieredstore.level2.dirs.path:
  'A comma-separated list of paths (eg., /mnt/ssd/alluxio/cache4,/mnt/hdd/alluxio/cache2,/mnt/hdd/alluxio/cache3) of storage directories for the third storage tier.'
alluxio.worker.tieredstore.level2.dirs.quota:
  'A comma-separated list of capacities (e.g., &quot;5GB,50GB,50GB&quot;) for each storage directory on the third storage tier specified by alluxio.worker.tieredstore.level2.dirs.path.'
alluxio.worker.tieredstore.level2.watermark.high.ratio:
  'The high watermark of the space in the third storage tier (a value between 0 and 1).'
alluxio.worker.tieredstore.level2.watermark.low.ratio:
  'The low watermark of the space in the third storage tier (a value between 0 and 1).'
alluxio.worker.tieredstore.levels:
  'The number of storage tiers on the worker.'
alluxio.worker.ufs.block.open.timeout:
  'Timeout to open a block from UFS.'
alluxio.worker.ufs.instream.cache.enabled:
  'Enable caching for seekable under storage input stream, so that subsequent seek operations on the same file will reuse the cached input stream. This will improve position read performance as the open operations of some under file system would be expensive. The cached input stream would be stale, when the UFS file is modified without notifying alluxio. '
alluxio.worker.ufs.instream.cache.expiration.time:
  'Cached UFS instream expiration time.'
alluxio.worker.ufs.instream.cache.max.size:
  'The max entries in the UFS instream cache.'
alluxio.worker.web.bind.host:
  'The hostname Alluxio worker''s web server binds to.'
alluxio.worker.web.hostname:
  'The hostname Alluxio worker''s web UI binds to.'
alluxio.worker.web.port:
  'The port Alluxio worker''s web UI runs on.'
alluxio.worker.whitelist:
  'A comma-separated list of prefixes of the paths which are cacheable, separated by semi-colons. Alluxio will try to cache the cacheable file when it is read for the first time.'
