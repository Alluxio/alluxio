<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2018-04-16
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop Amazon Web Services support &#x2013; Hadoop-AWS module: Integration with Amazon Web Services</title>
    <style type="text/css" media="all">
      @import url("../../css/maven-base.css");
      @import url("../../css/maven-theme.css");
      @import url("../../css/site.css");
    </style>
    <link rel="stylesheet" href="../../css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20180416" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                   <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
        &gt;
                  <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
        &gt;
                  <a href="../../index.html">Apache Hadoop Amazon Web Services support</a>
        &gt;
        Hadoop-AWS module: Integration with Amazon Web Services
        </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://git-wip-us.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
              
                                   &nbsp;| Last Published: 2018-04-16
              &nbsp;| Version: 2.9.1
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Compatibility.html">Hadoop Compatibility</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Hftp.html">HFTP</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html">Upgrade Domain</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html">DataNode Admin</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html">Router Federation</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html">Support for YARN Shared Cache</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainerExecutor.html">DockerContainerExecutor</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Running Applications in Docker Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/registry/index.html">Registry</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html">Graceful Decommission</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html">Opportunistic Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SharedCache.html">Shared Cache</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_v.2_REST_API">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-openstack/index.html">OpenStack Swift</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-resourceestimator/ResourceEstimator.html">Resource Estimator Service</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Benchmarking.html">Hadoop Benchmarking</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/release/index.html">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../../api/index.html">API docs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="../../images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--><h1>Hadoop-AWS module: Integration with Amazon Web Services</h1>

<ul>
<li><a href="#Overview">Overview</a>
<ul>
<li><a href="#Features">Features</a></li>
<li><a href="#Warning_1:_Object_Stores_are_not_filesystems">Warning #1: Object Stores are not filesystems</a></li>
<li><a href="#Warning_2:_Object_stores_dont_track_modification_times_of_directories">Warning #2: Object stores don&#x2019;t track modification times of directories</a></li>
<li><a href="#Warning_3:_Object_stores_have_differerent_authorization_models">Warning #3: Object stores have differerent authorization models</a></li>
<li><a href="#Warning_4:_Your_AWS_credentials_are_valuable">Warning #4: Your AWS credentials are valuable</a></li>
<li><a href="#Warning_5:_The_S3_client_provided_by_Amazon_EMR_are_not_from_the_Apache_Software_foundation_and_are_only_supported_by_Amazon.">Warning #5: The S3 client provided by Amazon EMR are not from the Apache Software foundation, and are only supported by Amazon.</a></li></ul></li>
<li><a href="#S3">S3</a>
<ul>
<li><a href="#Dependencies">Dependencies</a></li>
<li><a href="#Authentication_properties">Authentication properties</a></li></ul></li>
<li><a href="#S3N">S3N</a>
<ul>
<li><a href="#Features">Features</a></li>
<li><a href="#Dependencies">Dependencies</a></li>
<li><a href="#Authentication_properties">Authentication properties</a></li>
<li><a href="#Other_properties">Other properties</a></li></ul></li>
<li><a href="#S3A">S3A</a>
<ul>
<li><a href="#Features">Features</a></li>
<li><a href="#Dependencies">Dependencies</a></li>
<li><a href="#S3A_Authentication_methods">S3A Authentication methods</a></li>
<li><a href="#Authentication_properties">Authentication properties</a>
<ul>
<li><a href="#Authenticating_via_environment_variables">Authenticating via environment variables</a></li>
<li><a href="#Changing_Authentication_Providers">Changing Authentication Providers</a></li>
<li><a href="#Protecting_the_AWS_Credentials">Protecting the AWS Credentials</a>
<ul>
<li><a href="#Storing_secrets_with_Hadoop_Credential_Providers">Storing secrets with Hadoop Credential Providers</a>
<ul>
<li><a href="#Create_a_credential_file">Create a credential file</a></li>
<li><a href="#Configure_the_hadoop.security.credential.provider.path_property">Configure the hadoop.security.credential.provider.path property</a></li>
<li><a href="#Using_the_credentials">Using the credentials</a></li></ul></li></ul></li></ul></li>
<li><a href="#Other_properties">Other properties</a></li>
<li><a href="#Configuring_different_S3_buckets">Configuring different S3 buckets</a></li>
<li><a href="#Using_Per-Bucket_Configuration_to_access_data_round_the_world">Using Per-Bucket Configuration to access data round the world</a></li>
<li><a href="#Stabilizing:_S3A_Fast_Upload">Stabilizing: S3A Fast Upload</a>
<ul>
<li><a href="#Fast_Upload_with_Disk_Buffers_fs.s3a.fast.upload.bufferdisk">Fast Upload with Disk Buffers fs.s3a.fast.upload.buffer=disk</a></li>
<li><a href="#Fast_Upload_with_ByteBuffers:_fs.s3a.fast.upload.bufferbytebuffer">Fast Upload with ByteBuffers: fs.s3a.fast.upload.buffer=bytebuffer</a></li>
<li><a href="#Fast_Upload_with_Arrays:_fs.s3a.fast.upload.bufferarray">Fast Upload with Arrays: fs.s3a.fast.upload.buffer=array</a></li>
<li><a href="#S3A_Fast_Upload_Thread_Tuning">S3A Fast Upload Thread Tuning</a></li>
<li><a href="#Cleaning_up_After_Incremental_Upload_Failures:_fs.s3a.multipart.purge">Cleaning up After Incremental Upload Failures: fs.s3a.multipart.purge</a></li></ul></li>
<li><a href="#S3A_Experimental_fadvise_input_policy_support">S3A Experimental &#x201c;fadvise&#x201d; input policy support</a>
<ul>
<li><a href="#asequential_default">&#x201c;sequential&#x201d; (default)</a></li>
<li><a href="#anormal">&#x201c;normal&#x201d;</a></li>
<li><a href="#arandom">&#x201c;random&#x201d;</a></li></ul></li>
<li><a href="#Encrypting_objects_with_S3A"> Encrypting objects with S3A</a>
<ul>
<li><a href="#SSE-C_Warning">SSE-C Warning</a></li></ul></li></ul></li>
<li><a href="#Troubleshooting_S3A">Troubleshooting S3A</a>
<ul>
<li><a href="#ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem">ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem</a></li>
<li><a href="#ClassNotFoundException:_com.amazonaws.services.s3.AmazonS3Client">ClassNotFoundException: com.amazonaws.services.s3.AmazonS3Client</a></li>
<li><a href="#Missing_method_in_com.amazonaws_class">Missing method in com.amazonaws class</a></li>
<li><a href="#Missing_method_in_a_Jackson_class">Missing method in a Jackson class</a></li>
<li><a href="#Authentication_failure">Authentication failure</a>
<ul>
<li><a href="#Authentication_failure_due_to_clock_skew">Authentication failure due to clock skew</a></li>
<li><a href="#Authentication_failure_when_using_URLs_with_embedded_secrets">Authentication failure when using URLs with embedded secrets</a></li>
<li><a href="#Authentication_Failures_When_Running_on_Java_8u60">Authentication Failures When Running on Java 8u60+</a></li></ul></li>
<li><a href="#aBad_Request_exception_when_working_with_AWS_S3_Frankfurt_Seoul_or_other_V4_endpoint">&#x201c;Bad Request&#x201d; exception when working with AWS S3 Frankfurt, Seoul, or other &#x201c;V4&#x201d; endpoint</a></li>
<li><a href="#Error_message_The_bucket_you_are_attempting_to_access_must_be_addressed_using_the_specified_endpoint">Error message &#x201c;The bucket you are attempting to access must be addressed using the specified endpoint&#x201d;</a></li>
<li><a href="#aTimeout_waiting_for_connection_from_pool_when_writing_to_S3A">&#x201c;Timeout waiting for connection from pool&#x201d; when writing to S3A</a></li>
<li><a href="#aTimeout_waiting_for_connection_from_pool_when_reading_from_S3A">&#x201c;Timeout waiting for connection from pool&#x201d; when reading from S3A</a></li>
<li><a href="#Out_of_heap_memory_when_writing_to_S3A_via_Fast_Upload">Out of heap memory when writing to S3A via Fast Upload</a></li>
<li><a href="#When_writing_to_S3A:_java.io.FileNotFoundException:_Completing_multi-part_upload">When writing to S3A: &#x201c;java.io.FileNotFoundException: Completing multi-part upload&#x201d;</a></li>
<li><a href="#MultiObjectDeleteException_during_delete_or_rename_of_files">MultiObjectDeleteException during delete or rename of files</a></li>
<li><a href="#When_writing_to_S3A_HTTP_Exceptions_logged_at_info_from_AmazonHttpClient">When writing to S3A, HTTP Exceptions logged at info from AmazonHttpClient</a></li>
<li><a href="#Visible_S3_Inconsistency">Visible S3 Inconsistency</a></li>
<li><a href="#FileNotFoundException_even_though_the_file_was_just_written.">FileNotFoundException even though the file was just written.</a></li>
<li><a href="#File_not_found_in_a_directory_listing_even_though_getFileStatus_finds_it">File not found in a directory listing, even though getFileStatus() finds it</a></li>
<li><a href="#File_not_visiblesaved">File not visible/saved</a></li>
<li><a href="#File_flush_and_hflush_calls_do_not_save_data_to_S3A">File flush() and hflush() calls do not save data to S3A</a></li>
<li><a href="#S3_Server_Side_Encryption">S3 Server Side Encryption</a>
<ul>
<li><a href="#Using_SSE-KMS">Using SSE-KMS</a></li>
<li><a href="#Using_SSE-C">Using SSE-C</a></li></ul></li>
<li><a href="#Other_issues">Other issues</a></li></ul></li></ul>
<div class="section">
<h2><a name="Overview"></a>Overview</h2>
<p>The <tt>hadoop-aws</tt> module provides support for AWS integration. The generated JAR file, <tt>hadoop-aws.jar</tt> also declares a transitive dependency on all external artifacts which are needed for this support &#x2014;enabling downstream applications to easily use this support.</p>
<div class="section">
<h3><a name="Features"></a>Features</h3>

<ol style="list-style-type: decimal">
  
<li>The &#x201c;classic&#x201d; <tt>s3:</tt> filesystem for storing objects in Amazon S3 Storage. <b>NOTE: <tt>s3:</tt> is being phased out. Use <tt>s3n:</tt> or <tt>s3a:</tt> instead.</b></li>
  
<li>The second-generation, <tt>s3n:</tt> filesystem, making it easy to share data between hadoop and other applications via the S3 object store.</li>
  
<li>The third generation, <tt>s3a:</tt> filesystem. Designed to be a switch in replacement for <tt>s3n:</tt>, this filesystem binding supports larger files and promises higher performance.</li>
</ol>
<p>The specifics of using these filesystems are documented in this section.</p>
<p>See also:</p>

<ul>
  
<li><a href="testing.html">Testing</a></li>
  
<li><a href="troubleshooting_s3a.html">Troubleshooting S3a</a></li>
  
<li><a href="s3guard.html">S3Guard</a></li>
</ul></div>
<div class="section">
<h3><a name="Warning_1:_Object_Stores_are_not_filesystems"></a>Warning #1: Object Stores are not filesystems</h3>
<p>Amazon S3 is an example of &#x201c;an object store&#x201d;. In order to achieve scalability and especially high availability, S3 has &#x2014;as many other cloud object stores have done&#x2014; relaxed some of the constraints which classic &#x201c;POSIX&#x201d; filesystems promise.</p>
<p>Specifically</p>

<ol style="list-style-type: decimal">
  
<li>Files that are newly created from the Hadoop Filesystem APIs may not be immediately visible.</li>
  
<li>File delete and update operations may not immediately propagate. Old copies of the file may exist for an indeterminate time period.</li>
  
<li>Directory operations: <tt>delete()</tt> and <tt>rename()</tt> are implemented by recursive file-by-file operations. They take time at least proportional to the number of files, during which time partial updates may be visible. If the operations are interrupted, the filesystem is left in an intermediate state.</li>
</ol></div>
<div class="section">
<h3><a name="Warning_2:_Object_stores_dont_track_modification_times_of_directories"></a>Warning #2: Object stores don&#x2019;t track modification times of directories</h3>
<p>Features of Hadoop relying on this can have unexpected behaviour. E.g. the AggregatedLogDeletionService of YARN will not remove the appropriate logfiles.</p>
<p>For further discussion on these topics, please consult <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">The Hadoop FileSystem API Definition</a>.</p></div>
<div class="section">
<h3><a name="Warning_3:_Object_stores_have_differerent_authorization_models"></a>Warning #3: Object stores have differerent authorization models</h3>
<p>The object authorization model of S3 is much different from the file authorization model of HDFS and traditional file systems. It is not feasible to persist file ownership and permissions in S3, so S3A reports stub information from APIs that would query this metadata:</p>

<ul>
  
<li>File owner is reported as the current user.</li>
  
<li>File group also is reported as the current user. Prior to Apache Hadoop 2.8.0, file group was reported as empty (no group associated), which is a potential incompatibility problem for scripts that perform positional parsing of shell output and other clients that expect to find a well-defined group.</li>
  
<li>Directory permissions are reported as 777.</li>
  
<li>File permissions are reported as 666.</li>
</ul>
<p>S3A does not really enforce any authorization checks on these stub permissions. Users authenticate to an S3 bucket using AWS credentials. It&#x2019;s possible that object ACLs have been defined to enforce authorization at the S3 side, but this happens entirely within the S3 service, not within the S3A implementation.</p>
<p>For further discussion on these topics, please consult <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">The Hadoop FileSystem API Definition</a>.</p></div>
<div class="section">
<h3><a name="Warning_4:_Your_AWS_credentials_are_valuable"></a>Warning #4: Your AWS credentials are valuable</h3>
<p>Your AWS credentials not only pay for services, they offer read and write access to the data. Anyone with the credentials can not only read your datasets &#x2014;they can delete them.</p>
<p>Do not inadvertently share these credentials through means such as</p>

<ol style="list-style-type: decimal">
  
<li>Checking in to SCM any configuration files containing the secrets.</li>
  
<li>Logging them to a console, as they invariably end up being seen.</li>
  
<li>Defining filesystem URIs with the credentials in the URL, such as <tt>s3a://AK0010:secret@landsat/</tt>. They will end up in logs and error messages.</li>
  
<li>Including the secrets in bug reports.</li>
</ol>
<p>If you do any of these: change your credentials immediately!</p></div>
<div class="section">
<h3><a name="Warning_5:_The_S3_client_provided_by_Amazon_EMR_are_not_from_the_Apache_Software_foundation_and_are_only_supported_by_Amazon."></a>Warning #5: The S3 client provided by Amazon EMR are not from the Apache Software foundation, and are only supported by Amazon.</h3>
<p>Specifically: on Amazon EMR, s3a is not supported, and amazon recommend a different filesystem implementation. If you are using Amazon EMR, follow these instructions &#x2014;and be aware that all issues related to S3 integration in EMR can only be addressed by Amazon themselves: please raise your issues with them.</p></div></div>
<div class="section">
<h2><a name="S3"></a>S3</h2>
<p>The <tt>s3://</tt> filesystem is the original S3 store in the Hadoop codebase. It implements an inode-style filesystem atop S3, and was written to provide scaleability when S3 had significant limits on the size of blobs. It is incompatible with any other application&#x2019;s use of data in S3.</p>
<p>It is now deprecated and will be removed in Hadoop 3. Please do not use, and migrate off data which is on it.</p>
<div class="section">
<h3><a name="Dependencies"></a>Dependencies</h3>

<ul>
  
<li><tt>jets3t</tt> jar</li>
  
<li><tt>commons-codec</tt> jar</li>
  
<li><tt>commons-logging</tt> jar</li>
  
<li><tt>httpclient</tt> jar</li>
  
<li><tt>httpcore</tt> jar</li>
  
<li><tt>java-xmlbuilder</tt> jar</li>
</ul></div>
<div class="section">
<h3><a name="Authentication_properties"></a>Authentication properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3.awsAccessKeyId&lt;/name&gt;
  &lt;description&gt;AWS access key ID&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3.awsSecretAccessKey&lt;/name&gt;
  &lt;description&gt;AWS secret key&lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div></div>
<div class="section">
<h2><a name="S3N"></a>S3N</h2>
<p>S3N was the first S3 Filesystem client which used &#x201c;native&#x201d; S3 objects, hence the schema <tt>s3n://</tt>.</p>
<div class="section">
<h3><a name="Features"></a>Features</h3>

<ul>
  
<li>Directly reads and writes S3 objects.</li>
  
<li>Compatible with standard S3 clients.</li>
  
<li>Supports partitioned uploads for many-GB objects.</li>
  
<li>Available across all Hadoop 2.x releases.</li>
</ul>
<p>The S3N filesystem client, while widely used, is no longer undergoing active maintenance except for emergency security issues. There are known bugs, especially: it reads to end of a stream when closing a read; this can make <tt>seek()</tt> slow on large files. The reason there has been no attempt to fix this is that every upgrade of the Jets3t library, while fixing some problems, has unintentionally introduced new ones in either the changed Hadoop code, or somewhere in the Jets3t/Httpclient code base. The number of defects remained constant, they merely moved around.</p>
<p>By freezing the Jets3t jar version and avoiding changes to the code, we reduce the risk of making things worse.</p>
<p>The S3A filesystem client can read all files created by S3N. Accordingly it should be used wherever possible.</p></div>
<div class="section">
<h3><a name="Dependencies"></a>Dependencies</h3>

<ul>
  
<li><tt>jets3t</tt> jar</li>
  
<li><tt>commons-codec</tt> jar</li>
  
<li><tt>commons-logging</tt> jar</li>
  
<li><tt>httpclient</tt> jar</li>
  
<li><tt>httpcore</tt> jar</li>
  
<li><tt>java-xmlbuilder</tt> jar</li>
</ul></div>
<div class="section">
<h3><a name="Authentication_properties"></a>Authentication properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3n.awsAccessKeyId&lt;/name&gt;
  &lt;description&gt;AWS access key ID&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.awsSecretAccessKey&lt;/name&gt;
  &lt;description&gt;AWS secret key&lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="Other_properties"></a>Other properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3.buffer.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;
  &lt;description&gt;Determines where on the local filesystem the s3:/s3n: filesystem
  should store files before sending them to S3
  (or after retrieving them from S3).
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3.maxRetries&lt;/name&gt;
  &lt;value&gt;4&lt;/value&gt;
  &lt;description&gt;The maximum number of retries for reading or writing files to
    S3, before we signal failure to the application.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3.sleepTimeSeconds&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;
  &lt;description&gt;The number of seconds to sleep between each S3 retry.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.block.size&lt;/name&gt;
  &lt;value&gt;67108864&lt;/value&gt;
  &lt;description&gt;Block size to use when reading files using the native S3
  filesystem (s3n: URIs).&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.multipart.uploads.enabled&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Setting this property to true enables multiple uploads to
  native S3 filesystem. When uploading a file, it is split into blocks
  if the size is larger than fs.s3n.multipart.uploads.block.size.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.multipart.uploads.block.size&lt;/name&gt;
  &lt;value&gt;67108864&lt;/value&gt;
  &lt;description&gt;The block size for multipart uploads to native S3 filesystem.
  Default size is 64MB.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.multipart.copy.block.size&lt;/name&gt;
  &lt;value&gt;5368709120&lt;/value&gt;
  &lt;description&gt;The block size for multipart copy in native S3 filesystem.
  Default size is 5GB.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.server-side-encryption-algorithm&lt;/name&gt;
  &lt;value&gt;&lt;/value&gt;
  &lt;description&gt;Specify a server-side encryption algorithm for S3.
  Unset by default, and the only other currently allowable value is AES256.
  &lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div></div>
<div class="section">
<h2><a name="S3A"></a>S3A</h2>
<p>The S3A filesystem client, prefix <tt>s3a://</tt>, is the S3 client undergoing active development and maintenance. While this means that there is a bit of instability of configuration options and behavior, it also means that the code is getting better in terms of reliability, performance, monitoring and other features.</p>
<div class="section">
<h3><a name="Features"></a>Features</h3>

<ul>
  
<li>Directly reads and writes S3 objects.</li>
  
<li>Compatible with standard S3 clients.</li>
  
<li>Can read data created with S3N.</li>
  
<li>Can write data back that is readable by S3N. (Note: excluding encryption).</li>
  
<li>Supports partitioned uploads for many-GB objects.</li>
  
<li>Instrumented with Hadoop metrics.</li>
  
<li>Performance optimized operations, including <tt>seek()</tt> and <tt>readFully()</tt>.</li>
  
<li>Uses Amazon&#x2019;s Java S3 SDK with support for latest S3 features and authentication schemes.</li>
  
<li>Supports authentication via: environment variables, Hadoop configuration properties, the Hadoop key management store and IAM roles.</li>
  
<li>Supports S3 &#x201c;Server Side Encryption&#x201d; for both reading and writing.</li>
  
<li>Supports proxies</li>
  
<li>Test suites includes distcp and suites in downstream projects.</li>
  
<li>Available since Hadoop 2.6; considered production ready in Hadoop 2.7.</li>
  
<li>Actively maintained.</li>
  
<li>Supports per-bucket configuration.</li>
</ul>
<p>S3A is now the recommended client for working with S3 objects. It is also the one where patches for functionality and performance are very welcome.</p></div>
<div class="section">
<h3><a name="Dependencies"></a>Dependencies</h3>

<ul>
  
<li><tt>hadoop-aws</tt> jar.</li>
  
<li><tt>aws-java-sdk-s3</tt> jar.</li>
  
<li><tt>aws-java-sdk-core</tt> jar.</li>
  
<li><tt>aws-java-sdk-kms</tt> jar.</li>
  
<li><tt>joda-time</tt> jar; use version 2.8.1 or later.</li>
  
<li><tt>httpclient</tt> jar.</li>
  
<li>Jackson <tt>jackson-core</tt>, <tt>jackson-annotations</tt>, <tt>jackson-databind</tt> jars.</li>
</ul></div>
<div class="section">
<h3><a name="S3A_Authentication_methods"></a>S3A Authentication methods</h3>
<p>S3A supports multiple authentication mechanisms, and can be configured as to which mechanisms to use, and the order to use them. Custom implementations of <tt>com.amazonaws.auth.AWSCredentialsProvider</tt> may also be used.</p></div>
<div class="section">
<h3><a name="Authentication_properties"></a>Authentication properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.access.key&lt;/name&gt;
  &lt;description&gt;AWS access key ID.
   Omit for IAM role-based or provider-based authentication.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;
  &lt;description&gt;AWS secret key.
   Omit for IAM role-based or provider-based authentication.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;description&gt;
    Comma-separated class names of credential provider classes which implement
    com.amazonaws.auth.AWSCredentialsProvider.

    These are loaded and queried in sequence for a valid set of credentials.
    Each listed class must implement one of the following means of
    construction, which are attempted in order:
    1. a public constructor accepting java.net.URI and
        org.apache.hadoop.conf.Configuration,
    2. a public static method named getInstance that accepts no
       arguments and returns an instance of
       com.amazonaws.auth.AWSCredentialsProvider, or
    3. a public default constructor.

    Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows
    anonymous access to a publicly accessible S3 bucket without any credentials.
    Please note that allowing anonymous access to an S3 bucket compromises
    security and therefore is unsuitable for most use cases. It can be useful
    for accessing public data sets without requiring AWS credentials.

    If unspecified, then the default list of credential provider classes,
    queried in sequence, is:
    1. org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider: supports
        static configuration of AWS access key ID and secret access key.
        See also fs.s3a.access.key and fs.s3a.secret.key.
    2. com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports
        configuration of AWS access key ID and secret access key in
        environment variables named AWS_ACCESS_KEY_ID and
        AWS_SECRET_ACCESS_KEY, as documented in the AWS SDK.
    3. com.amazonaws.auth.InstanceProfileCredentialsProvider: supports use
        of instance profile credentials if running in an EC2 VM.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.session.token&lt;/name&gt;
  &lt;description&gt;
    Session token, when using org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
    as one of the providers.
  &lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<div class="section">
<h4><a name="Authenticating_via_environment_variables"></a>Authenticating via environment variables</h4>
<p>S3A supports configuration via <a class="externalLink" href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-environment">the standard AWS environment variables</a>.</p>
<p>The core environment variables are for the access key and associated secret:</p>

<div class="source">
<div class="source">
<pre>export AWS_ACCESS_KEY_ID=my.aws.key
export AWS_SECRET_ACCESS_KEY=my.secret.key
</pre></div></div>
<p>If the environment variable <tt>AWS_SESSION_TOKEN</tt> is set, session authentication using &#x201c;Temporary Security Credentials&#x201d; is enabled; the Key ID and secret key must be set to the credentials for that specific sesssion.</p>

<div class="source">
<div class="source">
<pre>export AWS_SESSION_TOKEN=SECRET-SESSION-TOKEN
export AWS_ACCESS_KEY_ID=SESSION-ACCESS-KEY
export AWS_SECRET_ACCESS_KEY=SESSION-SECRET-KEY
</pre></div></div>
<p>These environment variables can be used to set the authentication credentials instead of properties in the Hadoop configuration.</p>
<p><i>Important:</i> These environment variables are not propagated from client to server when YARN applications are launched. That is: having the AWS environment variables set when an application is launched will not permit the launched application to access S3 resources. The environment variables must (somehow) be set on the hosts/processes where the work is executed.</p></div>
<div class="section">
<h4><a name="Changing_Authentication_Providers"></a>Changing Authentication Providers</h4>
<p>The standard way to authenticate is with an access key and secret key using the properties in the configuration file.</p>
<p>The S3A client follows the following authentication chain:</p>

<ol style="list-style-type: decimal">
  
<li>If login details were provided in the filesystem URI, a warning is printed and then the username and password extracted for the AWS key and secret respectively.</li>
  
<li>The <tt>fs.s3a.access.key</tt> and <tt>fs.s3a.secret.key</tt> are looked for in the Hadoop XML configuration.</li>
  
<li>The <a class="externalLink" href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-environment">AWS environment variables</a>, are then looked for.</li>
  
<li>An attempt is made to query the Amazon EC2 Instance Metadata Service to  retrieve credentials published to EC2 VMs.</li>
</ol>
<p>S3A can be configured to obtain client authentication providers from classes which integrate with the AWS SDK by implementing the <tt>com.amazonaws.auth.AWSCredentialsProvider</tt> Interface. This is done by listing the implementation classes, in order of preference, in the configuration option <tt>fs.s3a.aws.credentials.provider</tt>.</p>
<p><i>Important</i>: AWS Credential Providers are distinct from <i>Hadoop Credential Providers</i>. As will be covered later, Hadoop Credential Providers allow passwords and other secrets to be stored and transferred more securely than in XML configuration files. AWS Credential Providers are classes which can be used by the Amazon AWS SDK to obtain an AWS login from a different source in the system, including environment variables, JVM properties and configuration files.</p>
<p>There are four AWS Credential Providers inside the <tt>hadoop-aws</tt> JAR:</p>

<table border="0" class="bodyTable">
  <thead>
    
<tr class="a">
      
<th>classname </th>
      
<th>description </th>
    </tr>
  </thead>
  <tbody>
    
<tr class="b">
      
<td><tt>org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider</tt></td>
      
<td>Session Credentials </td>
    </tr>
    
<tr class="a">
      
<td><tt>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</tt></td>
      
<td>Simple name/secret credentials </td>
    </tr>
    
<tr class="b">
      
<td><tt>org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider</tt></td>
      
<td>Shared instance of EC2 Metadata Credentials, which can reduce load on the EC2 instance metadata service. (See below.) </td>
    </tr>
    
<tr class="a">
      
<td><tt>org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider</tt></td>
      
<td>Anonymous Login </td>
    </tr>
  </tbody>
</table>
<p>There are also many in the Amazon SDKs, in particular two which are automatically set up in the authentication chain:</p>

<table border="0" class="bodyTable">
  <thead>
    
<tr class="a">
      
<th>classname </th>
      
<th>description </th>
    </tr>
  </thead>
  <tbody>
    
<tr class="b">
      
<td><tt>com.amazonaws.auth.InstanceProfileCredentialsProvider</tt></td>
      
<td>EC2 Metadata Credentials </td>
    </tr>
    
<tr class="a">
      
<td><tt>com.amazonaws.auth.EnvironmentVariableCredentialsProvider</tt></td>
      
<td>AWS Environment Variables </td>
    </tr>
  </tbody>
</table>
<p><i>EC2 Metadata Credentials with <tt>SharedInstanceProfileCredentialsProvider</tt></i></p>
<p>Applications running in EC2 may associate an IAM role with the VM and query the <a class="externalLink" href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">EC2 Instance Metadata Service</a> for credentials to access S3. Within the AWS SDK, this functionality is provided by <tt>InstanceProfileCredentialsProvider</tt>. In previous AWS SDK versions earlier than 1.11.39, heavily multi-threaded applications may trigger a high volume of calls to the instance metadata service and trigger throttling: either an HTTP 429 response or a forcible close of the connection.</p>
<p>To mitigate against that problem, <tt>hadoop-aws</tt> ships with a variant of <tt>InstanceProfileCredentialsProvider</tt> called <tt>SharedInstanceProfileCredentialsProvider</tt>. Using this ensures that all instances of S3A reuse the same instance profile credentials instead of issuing a large volume of redundant metadata service calls.</p>
<p>As of AWS SDK 1.11.39, the SDK code internally enforces a singleton. Hadoop has upgraded its dependency so that this class is deprecated. In next major version, this will be removed. If <tt>fs.s3a.aws.credentials.provider</tt> refers to <tt>org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider</tt>, S3A automatically uses <tt>com.amazonaws.auth.InstanceProfileCredentialsProvider</tt> instead, along with a warning message.</p>
<p><i>Session Credentials with <tt>TemporaryAWSCredentialsProvider</tt></i></p>
<p><a class="externalLink" href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html">Temporary Security Credentials</a> can be obtained from the Amazon Security Token Service; these consist of an access key, a secret key, and a session token.</p>
<p>To authenticate with these:</p>

<ol style="list-style-type: decimal">
  
<li>Declare <tt>org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider</tt> as the provider.</li>
  
<li>Set the session key in the property <tt>fs.s3a.session.token</tt>, and the access and secret key properties to those of this temporary session.</li>
</ol>
<p>Example:</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.access.key&lt;/name&gt;
  &lt;value&gt;SESSION-ACCESS-KEY&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;
  &lt;value&gt;SESSION-SECRET-KEY&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.session.token&lt;/name&gt;
  &lt;value&gt;SECRET-SESSION-TOKEN&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p>The lifetime of session credentials are fixed when the credentials are issued; once they expire the application will no longer be able to authenticate to AWS.</p>
<p><i>Anonymous Login with <tt>AnonymousAWSCredentialsProvider</tt></i></p>
<p>Specifying <tt>org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider</tt> allows anonymous access to a publicly accessible S3 bucket without any credentials. It can be useful for accessing public data sets without requiring AWS credentials.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p>Once this is done, there&#x2019;s no need to supply any credentials in the Hadoop configuration or via environment variables.</p>
<p>This option can be used to verify that an object store does not permit unauthenticated access: that is, if an attempt to list a bucket is made using the anonymous credentials, it should fail &#x2014;unless explicitly opened up for broader access.</p>

<div class="source">
<div class="source">
<pre>hadoop fs -ls \
 -D fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider \
 s3a://landsat-pds/
</pre></div></div>

<ol style="list-style-type: decimal">
  
<li>
<p>Allowing anonymous access to an S3 bucket compromises security and therefore is unsuitable for most use cases.</p></li>
  
<li>
<p>If a list of credential providers is given in <tt>fs.s3a.aws.credentials.provider</tt>, then the Anonymous Credential provider <i>must</i> come last. If not, credential providers listed after it will be ignored.</p></li>
</ol>
<p><i>Simple name/secret credentials with <tt>SimpleAWSCredentialsProvider</tt></i></p>
<p>This is is the standard credential provider, which supports the secret key in <tt>fs.s3a.access.key</tt> and token in <tt>fs.s3a.secret.key</tt> values. It does not support authentication with logins credentials declared in the URLs.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p>Apart from its lack of support of user:password details being included in filesystem URLs (a dangerous practise that is strongly discouraged), this provider acts exactly at the basic authenticator used in the default authentication chain.</p>
<p>This means that the default S3A authentication chain can be defined as</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;
  org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,
  com.amazonaws.auth.EnvironmentVariableCredentialsProvider,
  com.amazonaws.auth.InstanceProfileCredentialsProvider
  &lt;/value&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h4><a name="Protecting_the_AWS_Credentials"></a>Protecting the AWS Credentials</h4>
<p>To protect the access/secret keys from prying eyes, it is recommended that you use either IAM role-based authentication (such as EC2 instance profile) or the credential provider framework securely storing them and accessing them through configuration. The following describes using the latter for AWS credentials in the S3A FileSystem.</p>
<div class="section">
<h5><a name="Storing_secrets_with_Hadoop_Credential_Providers"></a>Storing secrets with Hadoop Credential Providers</h5>
<p>The Hadoop Credential Provider Framework allows secure &#x201c;Credential Providers&#x201d; to keep secrets outside Hadoop configuration files, storing them in encrypted files in local or Hadoop filesystems, and including them in requests.</p>
<p>The S3A configuration options with sensitive data (<tt>fs.s3a.secret.key</tt>, <tt>fs.s3a.access.key</tt> and <tt>fs.s3a.session.token</tt>) can have their data saved to a binary file stored, with the values being read in when the S3A filesystem URL is used for data access. The reference to this credential provider is all that is passed as a direct configuration option.</p>
<p>For additional reading on the Hadoop Credential Provider API see: <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>.</p>
<div class="section">
<h6><a name="Create_a_credential_file"></a>Create a credential file</h6>
<p>A credential file can be created on any Hadoop filesystem; when creating one on HDFS or a Unix filesystem the permissions are automatically set to keep the file private to the reader &#x2014;though as directory permissions are not touched, users should verify that the directory containing the file is readable only by the current user.</p>

<div class="source">
<div class="source">
<pre>hadoop credential create fs.s3a.access.key -value 123 \
    -provider jceks://hdfs@nn1.example.com:9001/user/backup/s3.jceks

hadoop credential create fs.s3a.secret.key -value 456 \
    -provider jceks://hdfs@nn1.example.com:9001/user/backup/s3.jceks
</pre></div></div>
<p>A credential file can be listed, to see what entries are kept inside it</p>

<div class="source">
<div class="source">
<pre>hadoop credential list -provider jceks://hdfs@nn1.example.com:9001/user/backup/s3.jceks

Listing aliases for CredentialProvider: jceks://hdfs@nn1.example.com:9001/user/backup/s3.jceks
fs.s3a.secret.key
fs.s3a.access.key
</pre></div></div>
<p>At this point, the credentials are ready for use.</p></div>
<div class="section">
<h6><a name="Configure_the_hadoop.security.credential.provider.path_property"></a>Configure the <tt>hadoop.security.credential.provider.path</tt> property</h6>
<p>The URL to the provider must be set in the configuration property <tt>hadoop.security.credential.provider.path</tt>, either on the command line or in XML configuration files.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;hadoop.security.credential.provider.path&lt;/name&gt;
  &lt;value&gt;jceks://hdfs@nn1.example.com:9001/user/backup/s3.jceks&lt;/value&gt;
  &lt;description&gt;Path to interrogate for protected credentials.&lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<p>Because this property only supplies the path to the secrets file, the configuration option itself is no longer a sensitive item.</p>
<p>The property <tt>hadoop.security.credential.provider.path</tt> is global to all filesystems and secrets. There is another property, <tt>fs.s3a.security.credential.provider.path</tt> which only lists credential providers for S3A filesystems. The two properties are combined into one, with the list of providers in the <tt>fs.s3a.</tt> property taking precedence over that of the <tt>hadoop.security</tt> list (i.e. they are prepended to the common list).</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.security.credential.provider.path&lt;/name&gt;
  &lt;value /&gt;
  &lt;description&gt;
    Optional comma separated list of credential providers, a list
    which is prepended to that set in hadoop.security.credential.provider.path
  &lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<p>Supporting a separate list in an <tt>fs.s3a.</tt> prefix permits per-bucket configuration of credential files.</p></div>
<div class="section">
<h6><a name="Using_the_credentials"></a>Using the credentials</h6>
<p>Once the provider is set in the Hadoop configuration, hadoop commands work exactly as if the secrets were in an XML file.</p>

<div class="source">
<div class="source">
<pre>hadoop distcp \
    hdfs://nn1.example.com:9001/user/backup/007020615 s3a://glacier1/

hadoop fs -ls s3a://glacier1/

</pre></div></div>
<p>The path to the provider can also be set on the command line:</p>

<div class="source">
<div class="source">
<pre>hadoop distcp \
    -D hadoop.security.credential.provider.path=jceks://hdfs@nn1.example.com:9001/user/backup/s3.jceks \
    hdfs://nn1.example.com:9001/user/backup/007020615 s3a://glacier1/

hadoop fs \
  -D fs.s3a.security.credential.provider.path=jceks://hdfs@nn1.example.com:9001/user/backup/s3.jceks \
  -ls s3a://glacier1/

</pre></div></div>
<p>Because the provider path is not itself a sensitive secret, there is no risk from placing its declaration on the command line.</p></div></div></div></div>
<div class="section">
<h3><a name="Other_properties"></a>Other properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.maximum&lt;/name&gt;
  &lt;value&gt;15&lt;/value&gt;
  &lt;description&gt;Controls the maximum number of simultaneous connections to S3.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.ssl.enabled&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;Enables or disables SSL connections to S3.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;
  &lt;description&gt;AWS S3 endpoint to connect to. An up-to-date list is
    provided in the AWS Documentation: regions and endpoints. Without this
    property, the standard region (s3.amazonaws.com) is assumed.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Enable S3 path style access ie disabling the default virtual hosting behaviour.
    Useful for S3A-compliant storage providers as it removes the need to set up DNS for virtual hosting.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.host&lt;/name&gt;
  &lt;description&gt;Hostname of the (optional) proxy server for S3 connections.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.port&lt;/name&gt;
  &lt;description&gt;Proxy server port. If this property is not set
    but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with
    the value of fs.s3a.connection.ssl.enabled).&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.username&lt;/name&gt;
  &lt;description&gt;Username for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.password&lt;/name&gt;
  &lt;description&gt;Password for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.domain&lt;/name&gt;
  &lt;description&gt;Domain for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.workstation&lt;/name&gt;
  &lt;description&gt;Workstation for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.attempts.maximum&lt;/name&gt;
  &lt;value&gt;20&lt;/value&gt;
  &lt;description&gt;How many times we should retry commands on transient errors.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.establish.timeout&lt;/name&gt;
  &lt;value&gt;5000&lt;/value&gt;
  &lt;description&gt;Socket connection setup timeout in milliseconds.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.timeout&lt;/name&gt;
  &lt;value&gt;200000&lt;/value&gt;
  &lt;description&gt;Socket connection timeout in milliseconds.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.paging.maximum&lt;/name&gt;
  &lt;value&gt;5000&lt;/value&gt;
  &lt;description&gt;How many keys to request from S3 when doing
     directory listings at a time.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.max&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;
  &lt;description&gt; Maximum number of concurrent active (part)uploads,
  which each use a thread from the threadpool.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.socket.send.buffer&lt;/name&gt;
  &lt;value&gt;8192&lt;/value&gt;
  &lt;description&gt;Socket send buffer hint to amazon connector. Represented in bytes.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.socket.recv.buffer&lt;/name&gt;
  &lt;value&gt;8192&lt;/value&gt;
  &lt;description&gt;Socket receive buffer hint to amazon connector. Represented in bytes.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.keepalivetime&lt;/name&gt;
  &lt;value&gt;60&lt;/value&gt;
  &lt;description&gt;Number of seconds a thread can be idle before being
    terminated.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.max.total.tasks&lt;/name&gt;
  &lt;value&gt;5&lt;/value&gt;
  &lt;description&gt;Number of (part)uploads allowed to the queue before
  blocking additional uploads.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.size&lt;/name&gt;
  &lt;value&gt;100M&lt;/value&gt;
  &lt;description&gt;How big (in bytes) to split upload or copy operations up into.
    A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.threshold&lt;/name&gt;
  &lt;value&gt;2147483647&lt;/value&gt;
  &lt;description&gt;How big (in bytes) to split upload or copy operations up into.
    This also controls the partition size in renamed files, as rename() involves
    copying the source file(s).
    A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multiobjectdelete.enable&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;When enabled, multiple single-object delete requests are replaced by
    a single 'delete multiple objects'-request, reducing the number of requests.
    Beware: legacy S3-compatible object stores might not support this request.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.acl.default&lt;/name&gt;
  &lt;description&gt;Set a canned ACL for newly created and copied objects. Value may be Private,
    PublicRead, PublicReadWrite, AuthenticatedRead, LogDeliveryWrite, BucketOwnerRead,
    or BucketOwnerFullControl.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;True if you want to purge existing multipart uploads that may not have been
     completed/aborted correctly&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge.age&lt;/name&gt;
  &lt;value&gt;86400&lt;/value&gt;
  &lt;description&gt;Minimum age in seconds of multipart uploads to purge&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.signing-algorithm&lt;/name&gt;
  &lt;description&gt;Override the default signing algorithm so legacy
    implementations can still be used&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.server-side-encryption-algorithm&lt;/name&gt;
  &lt;description&gt;Specify a server-side encryption algorithm for s3a: file system.
    Unset by default. It supports the following values: 'AES256' (for SSE-S3), 'SSE-KMS'
     and 'SSE-C'
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;fs.s3a.server-side-encryption.key&lt;/name&gt;
    &lt;description&gt;Specific encryption key to use if fs.s3a.server-side-encryption-algorithm
    has been set to 'SSE-KMS' or 'SSE-C'. In the case of SSE-C, the value of this property
    should be the Base64 encoded key. If you are using SSE-KMS and leave this property empty,
    you'll be using your default's S3 KMS key, otherwise you should set this property to
    the specific KMS key id.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.buffer.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
  &lt;description&gt;Comma separated list of directories that will be used to buffer file
    uploads to. No effect if fs.s3a.fast.upload is true.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.block.size&lt;/name&gt;
  &lt;value&gt;32M&lt;/value&gt;
  &lt;description&gt;Block size to use when reading files using s3a: file system.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.user.agent.prefix&lt;/name&gt;
  &lt;value&gt;&lt;/value&gt;
  &lt;description&gt;
    Sets a custom value that will be prepended to the User-Agent header sent in
    HTTP requests to the S3 back-end by S3AFileSystem.  The User-Agent header
    always includes the Hadoop version number followed by a string generated by
    the AWS SDK.  An example is &quot;User-Agent: Hadoop 2.8.0, aws-sdk-java/1.10.6&quot;.
    If this optional property is set, then its value is prepended to create a
    customized User-Agent.  For example, if this configuration property was set
    to &quot;MyApp&quot;, then an example of the resulting User-Agent would be
    &quot;User-Agent: MyApp, Hadoop 2.8.0, aws-sdk-java/1.10.6&quot;.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.impl&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.S3AFileSystem&lt;/value&gt;
  &lt;description&gt;The implementation class of the S3A Filesystem&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.AbstractFileSystem.s3a.impl&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.S3A&lt;/value&gt;
  &lt;description&gt;The implementation class of the S3A AbstractFileSystem.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.readahead.range&lt;/name&gt;
  &lt;value&gt;64K&lt;/value&gt;
  &lt;description&gt;Bytes to read ahead during a seek() before closing and
  re-opening the S3 HTTP connection. This option will be overridden if
  any call to setReadahead() is made to an open stream.&lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="Configuring_different_S3_buckets"></a>Configuring different S3 buckets</h3>
<p>Different S3 buckets can be accessed with different S3A client configurations. This allows for different endpoints, data read and write strategies, as well as login details.</p>

<ol style="list-style-type: decimal">
  
<li>All <tt>fs.s3a</tt> options other than a small set of unmodifiable values  (currently <tt>fs.s3a.impl</tt>) can be set on a per bucket basis.</li>
  
<li>The bucket specific option is set by replacing the <tt>fs.s3a.</tt> prefix on an option with <tt>fs.s3a.bucket.BUCKETNAME.</tt>, where <tt>BUCKETNAME</tt> is the name of the bucket.</li>
  
<li>When connecting to a bucket, all options explicitly set will override the base <tt>fs.s3a.</tt> values.</li>
</ol>
<p>As an example, a configuration could have a base configuration to use the IAM role information available when deployed in Amazon EC2.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;com.amazonaws.auth.InstanceProfileCredentialsProvider&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p>This will be the default authentication mechanism for S3A buckets.</p>
<p>A bucket <tt>s3a://nightly/</tt> used for nightly data uses a session key:</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.bucket.nightly.access.key&lt;/name&gt;
  &lt;value&gt;AKAACCESSKEY-2&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.bucket.nightly.secret.key&lt;/name&gt;
  &lt;value&gt;SESSIONSECRETKEY&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.bucket.nightly.session.token&lt;/name&gt;
  &lt;value&gt;Short-lived-session-token&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.bucket.nightly.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p>Finally, the public <tt>s3a://landsat-pds/</tt> bucket is accessed anonymously:</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.bucket.landsat-pds.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p><b>Customizing S3A secrets held in credential files</b></p>
<p>Although most properties are automatically propagated from their <tt>fs.s3a.bucket.</tt>-prefixed custom entry to that of the base <tt>fs.s3a.</tt> option supporting secrets kept in Hadoop credential files is slightly more complex. This is because the property values are kept in these files, and cannot be dynamically patched.</p>
<p>Instead, callers need to create different configuration files for each bucket, setting the base secrets (<tt>fs.s3a.access.key</tt>, etc), then declare the path to the appropriate credential file in a bucket-specific version of the property <tt>fs.s3a.security.credential.provider.path</tt>.</p></div>
<div class="section">
<h3><a name="Using_Per-Bucket_Configuration_to_access_data_round_the_world"></a>Using Per-Bucket Configuration to access data round the world</h3>
<p>S3 Buckets are hosted in different &#x201c;regions&#x201d;, the default being &#x201c;US-East&#x201d;. The S3A client talks to this region by default, issing HTTP requests to the server <tt>s3.amazonaws.com</tt>.</p>
<p>S3A can work with buckets from any region. Each region has its own S3 endpoint, documented <a class="externalLink" href="http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region">by Amazon</a>.</p>

<ol style="list-style-type: decimal">
  
<li>Applications running in EC2 infrastructure do not pay for IO to/from <i>local S3 buckets</i>. They will be billed for access to remote buckets. Always use local buckets and local copies of data, wherever possible.</li>
  
<li>The default S3 endpoint can support data IO with any bucket when the V1 request signing protocol is used.</li>
  
<li>When the V4 signing protocol is used, AWS requires the explicit region endpoint to be used &#x2014;hence S3A must be configured to use the specific endpoint. This is done in the configuration option <tt>fs.s3a.endpoint</tt>.</li>
  
<li>All endpoints other than the default endpoint only support interaction with buckets local to that S3 instance.</li>
</ol>
<p>While it is generally simpler to use the default endpoint, working with V4-signing-only regions (Frankfurt, Seoul) requires the endpoint to be identified. Expect better performance from direct connections &#x2014;traceroute will give you some insight.</p>
<p>If the wrong endpoint is used, the request may fail. This may be reported as a 301/redirect error, or as a 400 Bad Request: take these as cues to check the endpoint setting of a bucket.</p>
<p>Here is a list of properties defining all AWS S3 regions, current as of June 2017:</p>

<div class="source">
<div class="source">
<pre>&lt;!--
 This is the default endpoint, which can be used to interact
 with any v2 region.
 --&gt;
&lt;property&gt;
  &lt;name&gt;central.endpoint&lt;/name&gt;
  &lt;value&gt;s3.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;canada.endpoint&lt;/name&gt;
  &lt;value&gt;s3.ca-central-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;frankfurt.endpoint&lt;/name&gt;
  &lt;value&gt;s3.eu-central-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;ireland.endpoint&lt;/name&gt;
  &lt;value&gt;s3-eu-west-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;london.endpoint&lt;/name&gt;
  &lt;value&gt;s3.eu-west-2.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mumbai.endpoint&lt;/name&gt;
  &lt;value&gt;s3.ap-south-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;ohio.endpoint&lt;/name&gt;
  &lt;value&gt;s3.us-east-2.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;oregon.endpoint&lt;/name&gt;
  &lt;value&gt;s3-us-west-2.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;sao-paolo.endpoint&lt;/name&gt;
  &lt;value&gt;s3-sa-east-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;seoul.endpoint&lt;/name&gt;
  &lt;value&gt;s3.ap-northeast-2.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;singapore.endpoint&lt;/name&gt;
  &lt;value&gt;s3-ap-southeast-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;sydney.endpoint&lt;/name&gt;
  &lt;value&gt;s3-ap-southeast-2.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;tokyo.endpoint&lt;/name&gt;
  &lt;value&gt;s3-ap-northeast-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;virginia.endpoint&lt;/name&gt;
  &lt;value&gt;${central.endpoint}&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p>This list can be used to specify the endpoint of individual buckets, for example for buckets in the central and EU/Ireland endpoints.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.bucket.landsat-pds.endpoint&lt;/name&gt;
  &lt;value&gt;${central.endpoint}&lt;/value&gt;
  &lt;description&gt;The endpoint for s3a://landsat-pds URLs&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.bucket.eu-dataset.endpoint&lt;/name&gt;
  &lt;value&gt;${ireland.endpoint}&lt;/value&gt;
  &lt;description&gt;The endpoint for s3a://eu-dataset URLs&lt;/description&gt;
&lt;/property&gt;

</pre></div></div>
<p>Why explicitly declare a bucket bound to the central endpoint? It ensures that if the default endpoint is changed to a new region, data store in US-east is still reachable.</p></div>
<div class="section">
<h3><a name="Stabilizing:_S3A_Fast_Upload"></a><a name="s3a_fast_upload"></a>Stabilizing: S3A Fast Upload</h3>
<p><b>New in Hadoop 2.7; significantly enhanced in Hadoop 2.8</b></p>
<p>Because of the nature of the S3 object store, data written to an S3A <tt>OutputStream</tt> is not written incrementally &#x2014;instead, by default, it is buffered to disk until the stream is closed in its <tt>close()</tt> method.</p>
<p>This can make output slow:</p>

<ul>
  
<li>The execution time for <tt>OutputStream.close()</tt> is proportional to the amount of data buffered and inversely proportional to the bandwidth. That is <tt>O(data/bandwidth)</tt>.</li>
  
<li>The bandwidth is that available from the host to S3: other work in the same process, server or network at the time of upload may increase the upload time, hence the duration of the <tt>close()</tt> call.</li>
  
<li>If a process uploading data fails before <tt>OutputStream.close()</tt> is called, all data is lost.</li>
  
<li>The disks hosting temporary directories defined in <tt>fs.s3a.buffer.dir</tt> must have the capacity to store the entire buffered file.</li>
</ul>
<p>Put succinctly: the further the process is from the S3 endpoint, or the smaller the EC-hosted VM is, the longer it will take work to complete.</p>
<p>This can create problems in application code:</p>

<ul>
  
<li>Code often assumes that the <tt>close()</tt> call is fast;  the delays can create bottlenecks in operations.</li>
  
<li>Very slow uploads sometimes cause applications to time out. (generally, threads blocking during the upload stop reporting progress, so trigger timeouts)</li>
  
<li>Streaming very large amounts of data may consume all disk space before the upload begins.</li>
</ul>
<p>Work to addess this began in Hadoop 2.7 with the <tt>S3AFastOutputStream</tt> <a class="externalLink" href="https://issues.apache.org/jira/browse/HADOOP-11183">HADOOP-11183</a>, and has continued with <tt>S3ABlockOutputStream</tt> <a class="externalLink" href="https://issues.apache.org/jira/browse/HADOOP-13560">HADOOP-13560</a>.</p>
<p>This adds an alternative output stream, &#x201c;S3a Fast Upload&#x201d; which:</p>

<ol style="list-style-type: decimal">
  
<li>Always uploads large files as blocks with the size set by <tt>fs.s3a.multipart.size</tt>. That is: the threshold at which multipart uploads begin and the size of each upload are identical.</li>
  
<li>Buffers blocks to disk (default) or in on-heap or off-heap memory.</li>
  
<li>Uploads blocks in parallel in background threads.</li>
  
<li>Begins uploading blocks as soon as the buffered data exceeds this partition size.</li>
  
<li>When buffering data to disk, uses the directory/directories listed in <tt>fs.s3a.buffer.dir</tt>. The size of data which can be buffered is limited to the available disk space.</li>
  
<li>Generates output statistics as metrics on the filesystem, including statistics of active and pending block uploads.</li>
  
<li>Has the time to <tt>close()</tt> set by the amount of remaning data to upload, rather than the total size of the file.</li>
</ol>
<p>With incremental writes of blocks, &#x201c;S3A fast upload&#x201d; offers an upload time at least as fast as the &#x201c;classic&#x201d; mechanism, with significant benefits on long-lived output streams, and when very large amounts of data are generated. The in memory buffering mechanims may also offer speedup when running adjacent to S3 endpoints, as disks are not used for intermediate data storage.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;
    Use the incremental block upload mechanism with
    the buffering mechanism set in fs.s3a.fast.upload.buffer.
    The number of threads performing uploads in the filesystem is defined
    by fs.s3a.threads.max; the queue of waiting uploads limited by
    fs.s3a.max.total.tasks.
    The size of each buffer is set by fs.s3a.multipart.size.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.buffer&lt;/name&gt;
  &lt;value&gt;disk&lt;/value&gt;
  &lt;description&gt;
    The buffering mechanism to use when using S3A fast upload
    (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer.
    This configuration option has no effect if fs.s3a.fast.upload is false.

    &quot;disk&quot; will use the directories listed in fs.s3a.buffer.dir as
    the location(s) to save data prior to being uploaded.

    &quot;array&quot; uses arrays in the JVM heap

    &quot;bytebuffer&quot; uses off-heap memory within the JVM.

    Both &quot;array&quot; and &quot;bytebuffer&quot; will consume memory in a single stream up to the number
    of blocks set by:

        fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks.

    If using either of these mechanisms, keep this value low

    The total number of threads performing work across all threads is set by
    fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued
    work items.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.size&lt;/name&gt;
  &lt;value&gt;100M&lt;/value&gt;
  &lt;description&gt;How big (in bytes) to split upload or copy operations up into.
    A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.active.blocks&lt;/name&gt;
  &lt;value&gt;8&lt;/value&gt;
  &lt;description&gt;
    Maximum Number of blocks a single output stream can have
    active (uploading, or queued to the central FileSystem
    instance's pool of queued operations.

    This stops a single stream overloading the shared thread pool.
  &lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<p><b>Notes</b></p>

<ul>
  
<li>
<p>If the amount of data written to a stream is below that set in <tt>fs.s3a.multipart.size</tt>, the upload is performed in the <tt>OutputStream.close()</tt> operation &#x2014;as with the original output stream.</p></li>
  
<li>
<p>The published Hadoop metrics monitor include live queue length and upload operation counts, so identifying when there is a backlog of work/ a mismatch between data generation rates and network bandwidth. Per-stream statistics can also be logged by calling <tt>toString()</tt> on the current stream.</p></li>
  
<li>
<p>Incremental writes are not visible; the object can only be listed or read when the multipart operation completes in the <tt>close()</tt> call, which will block until the upload is completed.</p></li>
</ul>
<div class="section">
<h4><a name="Fast_Upload_with_Disk_Buffers_fs.s3a.fast.upload.bufferdisk"></a><a name="s3a_fast_upload_disk"></a>Fast Upload with Disk Buffers <tt>fs.s3a.fast.upload.buffer=disk</tt></h4>
<p>When <tt>fs.s3a.fast.upload.buffer</tt> is set to <tt>disk</tt>, all data is buffered to local hard disks prior to upload. This minimizes the amount of memory consumed, and so eliminates heap size as the limiting factor in queued uploads &#x2014;exactly as the original &#x201c;direct to disk&#x201d; buffering used when <tt>fs.s3a.fast.upload=false</tt>.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.buffer&lt;/name&gt;
  &lt;value&gt;disk&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.buffer.dir&lt;/name&gt;
  &lt;value&gt;&lt;/value&gt;
  &lt;description&gt;Comma separated list of temporary directories use for
  storing blocks of data prior to their being uploaded to S3.
  When unset, the Hadoop temporary directory hadoop.tmp.dir is used&lt;/description&gt;
&lt;/property&gt;

</pre></div></div>
<p>This is the default buffer mechanism. The amount of data which can be buffered is limited by the amount of available disk space.</p></div>
<div class="section">
<h4><a name="Fast_Upload_with_ByteBuffers:_fs.s3a.fast.upload.bufferbytebuffer"></a><a name="s3a_fast_upload_bytebuffer"></a>Fast Upload with ByteBuffers: <tt>fs.s3a.fast.upload.buffer=bytebuffer</tt></h4>
<p>When <tt>fs.s3a.fast.upload.buffer</tt> is set to <tt>bytebuffer</tt>, all data is buffered in &#x201c;Direct&#x201d; ByteBuffers prior to upload. This <i>may</i> be faster than buffering to disk, and, if disk space is small (for example, tiny EC2 VMs), there may not be much disk space to buffer with.</p>
<p>The ByteBuffers are created in the memory of the JVM, but not in the Java Heap itself. The amount of data which can be buffered is limited by the Java runtime, the operating system, and, for YARN applications, the amount of memory requested for each container.</p>
<p>The slower the upload bandwidth to S3, the greater the risk of running out of memory &#x2014;and so the more care is needed in <a href="#s3a_fast_upload_thread_tuning">tuning the upload settings</a>.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.buffer&lt;/name&gt;
  &lt;value&gt;bytebuffer&lt;/value&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h4><a name="Fast_Upload_with_Arrays:_fs.s3a.fast.upload.bufferarray"></a><a name="s3a_fast_upload_array"></a>Fast Upload with Arrays: <tt>fs.s3a.fast.upload.buffer=array</tt></h4>
<p>When <tt>fs.s3a.fast.upload.buffer</tt> is set to <tt>array</tt>, all data is buffered in byte arrays in the JVM&#x2019;s heap prior to upload. This <i>may</i> be faster than buffering to disk.</p>
<p>This <tt>array</tt> option is similar to the in-memory-only stream offered in Hadoop 2.7 with <tt>fs.s3a.fast.upload=true</tt></p>
<p>The amount of data which can be buffered is limited by the available size of the JVM heap heap. The slower the write bandwidth to S3, the greater the risk of heap overflows. This risk can be mitigated by <a href="#s3a_fast_upload_thread_tuning">tuning the upload settings</a>.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.buffer&lt;/name&gt;
  &lt;value&gt;array&lt;/value&gt;
&lt;/property&gt;

</pre></div></div></div>
<div class="section">
<h4><a name="S3A_Fast_Upload_Thread_Tuning"></a><a name="s3a_fast_upload_thread_tuning"></a>S3A Fast Upload Thread Tuning</h4>
<p>Both the <a href="#s3a_fast_upload_array">Array</a> and <a href="#s3a_fast_upload_bytebuffer">Byte buffer</a> buffer mechanisms can consume very large amounts of memory, on-heap or off-heap respectively. The <a href="#s3a_fast_upload_disk">disk buffer</a> mechanism does not use much memory up, but will consume hard disk capacity.</p>
<p>If there are many output streams being written to in a single process, the amount of memory or disk used is the multiple of all stream&#x2019;s active memory/disk use.</p>
<p>Careful tuning may be needed to reduce the risk of running out memory, especially if the data is buffered in memory.</p>
<p>There are a number parameters which can be tuned:</p>

<ol style="list-style-type: decimal">
  
<li>
<p>The total number of threads available in the filesystem for data uploads <i>or any other queued filesystem operation</i>. This is set in <tt>fs.s3a.threads.max</tt></p></li>
  
<li>
<p>The number of operations which can be queued for execution:, <i>awaiting a thread</i>: <tt>fs.s3a.max.total.tasks</tt></p></li>
  
<li>
<p>The number of blocks which a single output stream can have active, that is: being uploaded by a thread, or queued in the filesystem thread queue: <tt>fs.s3a.fast.upload.active.blocks</tt></p></li>
  
<li>
<p>How long an idle thread can stay in the thread pool before it is retired: <tt>fs.s3a.threads.keepalivetime</tt></p></li>
</ol>
<p>When the maximum allowed number of active blocks of a single stream is reached, no more blocks can be uploaded from that stream until one or more of those active blocks&#x2019; uploads completes. That is: a <tt>write()</tt> call which would trigger an upload of a now full datablock, will instead block until there is capacity in the queue.</p>
<p>How does that come together?</p>

<ul>
  
<li>
<p>As the pool of threads set in <tt>fs.s3a.threads.max</tt> is shared (and intended to be used across all threads), a larger number here can allow for more parallel operations. However, as uploads require network bandwidth, adding more threads does not guarantee speedup.</p></li>
  
<li>
<p>The extra queue of tasks for the thread pool (<tt>fs.s3a.max.total.tasks</tt>) covers all ongoing background S3A operations (future plans include: parallelized rename operations, asynchronous directory operations).</p></li>
  
<li>
<p>When using memory buffering, a small value of <tt>fs.s3a.fast.upload.active.blocks</tt> limits the amount of memory which can be consumed per stream.</p></li>
  
<li>
<p>When using disk buffering a larger value of <tt>fs.s3a.fast.upload.active.blocks</tt> does not consume much memory. But it may result in a large number of blocks to compete with other filesystem operations.</p></li>
</ul>
<p>We recommend a low value of <tt>fs.s3a.fast.upload.active.blocks</tt>; enough to start background upload without overloading other parts of the system, then experiment to see if higher values deliver more throughtput &#x2014;especially from VMs running on EC2.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.active.blocks&lt;/name&gt;
  &lt;value&gt;4&lt;/value&gt;
  &lt;description&gt;
    Maximum Number of blocks a single output stream can have
    active (uploading, or queued to the central FileSystem
    instance's pool of queued operations.

    This stops a single stream overloading the shared thread pool.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.max&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;
  &lt;description&gt;The total number of threads available in the filesystem for data
    uploads *or any other queued filesystem operation*.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.max.total.tasks&lt;/name&gt;
  &lt;value&gt;5&lt;/value&gt;
  &lt;description&gt;The number of operations which can be queued for execution&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.keepalivetime&lt;/name&gt;
  &lt;value&gt;60&lt;/value&gt;
  &lt;description&gt;Number of seconds a thread can be idle before being
    terminated.&lt;/description&gt;
&lt;/property&gt;

</pre></div></div></div>
<div class="section">
<h4><a name="Cleaning_up_After_Incremental_Upload_Failures:_fs.s3a.multipart.purge"></a><a name="s3a_multipart_purge"></a>Cleaning up After Incremental Upload Failures: <tt>fs.s3a.multipart.purge</tt></h4>
<p>If an incremental streaming operation is interrupted, there may be intermediate partitions uploaded to S3 &#x2014;data which will be billed for.</p>
<p>These charges can be reduced by enabling <tt>fs.s3a.multipart.purge</tt>, and setting a purge time in seconds, such as 86400 seconds &#x2014;24 hours. When an S3A FileSystem instance is instantiated with the purge time greater than zero, it will, on startup, delete all outstanding partition requests older than this time.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;True if you want to purge existing multipart uploads that may not have been
     completed/aborted correctly&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge.age&lt;/name&gt;
  &lt;value&gt;86400&lt;/value&gt;
  &lt;description&gt;Minimum age in seconds of multipart uploads to purge&lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<p>If an S3A client is instantited with <tt>fs.s3a.multipart.purge=true</tt>, it will delete all out of date uploads <i>in the entire bucket</i>. That is: it will affect all multipart uploads to that bucket, from all applications.</p>
<p>Leaving <tt>fs.s3a.multipart.purge</tt> to its default, <tt>false</tt>, means that the client will not make any attempt to reset or change the partition rate.</p>
<p>The best practise for using this option is to disable multipart purges in normal use of S3A, enabling only in manual/scheduled housekeeping operations.</p></div></div>
<div class="section">
<h3><a name="S3A_Experimental_fadvise_input_policy_support"></a>S3A Experimental &#x201c;fadvise&#x201d; input policy support</h3>
<p><b>Warning: EXPERIMENTAL: behavior may change in future</b></p>
<p>The S3A Filesystem client supports the notion of input policies, similar to that of the Posix <tt>fadvise()</tt> API call. This tunes the behavior of the S3A client to optimise HTTP GET requests for the different use cases.</p>
<div class="section">
<h4><a name="asequential_default"></a>&#x201c;sequential&#x201d; (default)</h4>
<p>Read through the file, possibly with some short forward seeks.</p>
<p>The whole document is requested in a single HTTP request; forward seeks within the readahead range are supported by skipping over the intermediate data.</p>
<p>This is leads to maximum read throughput &#x2014;but with very expensive backward seeks.</p></div>
<div class="section">
<h4><a name="anormal"></a>&#x201c;normal&#x201d;</h4>
<p>This is currently the same as &#x201c;sequential&#x201d;.</p></div>
<div class="section">
<h4><a name="arandom"></a>&#x201c;random&#x201d;</h4>
<p>Optimised for random IO, specifically the Hadoop <tt>PositionedReadable</tt> operations &#x2014;though <tt>seek(offset); read(byte_buffer)</tt> also benefits.</p>
<p>Rather than ask for the whole file, the range of the HTTP request is set to that that of the length of data desired in the <tt>read</tt> operation (Rounded up to the readahead value set in <tt>setReadahead()</tt> if necessary).</p>
<p>By reducing the cost of closing existing HTTP requests, this is highly efficient for file IO accessing a binary file through a series of <tt>PositionedReadable.read()</tt> and <tt>PositionedReadable.readFully()</tt> calls. Sequential reading of a file is expensive, as now many HTTP requests must be made to read through the file.</p>
<p>For operations simply reading through a file: copying, distCp, reading Gzipped or other compressed formats, parsing .csv files, etc, the <tt>sequential</tt> policy is appropriate. This is the default: S3A does not need to be configured.</p>
<p>For the specific case of high-performance random access IO, the <tt>random</tt> policy may be considered. The requirements are:</p>

<ul>
  
<li>Data is read using the <tt>PositionedReadable</tt> API.</li>
  
<li>Long distance (many MB) forward seeks</li>
  
<li>Backward seeks as likely as forward seeks.</li>
  
<li>Little or no use of single character <tt>read()</tt> calls or small <tt>read(buffer)</tt> calls.</li>
  
<li>Applications running close to the S3 data store. That is: in EC2 VMs in the same datacenter as the S3 instance.</li>
</ul>
<p>The desired fadvise policy must be set in the configuration option <tt>fs.s3a.experimental.input.fadvise</tt> when the filesystem instance is created. That is: it can only be set on a per-filesystem basis, not on a per-file-read basis.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.experimental.input.fadvise&lt;/name&gt;
  &lt;value&gt;random&lt;/value&gt;
  &lt;description&gt;Policy for reading files.
   Values: 'random', 'sequential' or 'normal'
   &lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<p><a class="externalLink" href="https://issues.apache.org/jira/browse/HDFS-2744">HDFS-2744</a>, <i>Extend FSDataInputStream to allow fadvise</i> proposes adding a public API to set fadvise policies on input streams. Once implemented, this will become the supported mechanism used for configuring the input IO policy.</p></div></div>
<div class="section">
<h3><a name="Encrypting_objects_with_S3A"></a><a name="s3a_encryption"></a> Encrypting objects with S3A</h3>
<p>Currently, S3A only supports S3&#x2019;s Server Side Encryption for at rest data encryption. It is <i>encouraged</i> to read up on the <a class="externalLink" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html">AWS documentation</a> for S3 Server Side Encryption before using these options as each behave differently and the documentation will be more up to date on its behavior. When configuring an encryption method in the <tt>core-site.xml</tt>, this will apply cluster wide. Any new files written will be encrypted with this encryption configuration. Any existing files when read, will decrypt using the existing method (if possible) and will not be re-encrypted with the new method. It is also possible if mixing multiple keys that the user does not have access to decrypt the object. It is <b>NOT</b> advised to mix and match encryption types in a bucket, and is <i>strongly</i> recommended to just one type and key per bucket.</p>
<p>SSE-S3 is where S3 will manage the encryption keys for each object. The parameter for <tt>fs.s3a.server-side-encryption-algorithm</tt> is <tt>AES256</tt>.</p>
<p>SSE-KMS is where the user specifies a Customer Master Key(CMK) that is used to encrypt the objects. The user may specify a specific CMK or leave the <tt>fs.s3a.server-side-encryption.key</tt> empty to use the default auto-generated key in AWS IAM. Each CMK configured in AWS IAM is region specific, and cannot be used in a in a S3 bucket in a different region. There is can also be policies assigned to the CMK that prohibit or restrict its use for users causing S3A requests to fail.</p>
<p>SSE-C is where the user specifies an actual base64 encoded AES-256 key supplied and managed by the user.</p>
<div class="section">
<h4><a name="SSE-C_Warning"></a>SSE-C Warning</h4>
<p>It is strongly recommended to fully understand how SSE-C works in the S3 environment before using this encryption type. Please refer to the Server Side Encryption documentation available from AWS. SSE-C is only recommended for advanced users with advanced encryption use cases. Failure to properly manage encryption keys can cause data loss. Currently, the AWS S3 API(and thus S3A) only supports one encryption key and cannot support decrypting objects during moves under a previous key to a new destination. It is <b>NOT</b> advised to use multiple encryption keys in a bucket, and is recommended to use one key per bucket and to not change this key. This is due to when a request is made to S3, the actual encryption key must be provided to decrypt the object and access the metadata. Since only one encryption key can be provided at a time, S3A will not pass the correct encryption key to decrypt the data. Please see the troubleshooting section for more information.</p></div></div></div>
<div class="section">
<h2><a name="Troubleshooting_S3A"></a>Troubleshooting S3A</h2>
<p>Common problems working with S3A are</p>

<ol style="list-style-type: decimal">
  
<li>Classpath</li>
  
<li>Authentication</li>
  
<li>S3 Inconsistency side-effects</li>
</ol>
<p>Classpath is usually the first problem. For the S3x filesystem clients, you need the Hadoop-specific filesystem clients, third party S3 client libraries compatible with the Hadoop code, and any dependent libraries compatible with Hadoop and the specific JVM.</p>
<p>The classpath must be set up for the process talking to S3: if this is code running in the Hadoop cluster, the JARs must be on that classpath. That includes <tt>distcp</tt>.</p>
<div class="section">
<h3><a name="ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem"></a><tt>ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem</tt></h3>
<p>(or <tt>org.apache.hadoop.fs.s3native.NativeS3FileSystem</tt>, <tt>org.apache.hadoop.fs.s3.S3FileSystem</tt>).</p>
<p>These are the Hadoop classes, found in the <tt>hadoop-aws</tt> JAR. An exception reporting one of these classes is missing means that this JAR is not on the classpath.</p></div>
<div class="section">
<h3><a name="ClassNotFoundException:_com.amazonaws.services.s3.AmazonS3Client"></a><tt>ClassNotFoundException: com.amazonaws.services.s3.AmazonS3Client</tt></h3>
<p>(or other <tt>com.amazonaws</tt> class.)</p>
<p>This means that one or more of the <tt>aws-*-sdk</tt> JARs are missing. Add them.</p></div>
<div class="section">
<h3><a name="Missing_method_in_com.amazonaws_class"></a>Missing method in <tt>com.amazonaws</tt> class</h3>
<p>This can be triggered by incompatibilities between the AWS SDK on the classpath and the version which Hadoop was compiled with.</p>
<p>The AWS SDK JARs change their signature enough between releases that the only way to safely update the AWS SDK version is to recompile Hadoop against the later version.</p>
<p>There&#x2019;s nothing the Hadoop team can do here: if you get this problem, then sorry, but you are on your own. The Hadoop developer team did look at using reflection to bind to the SDK, but there were too many changes between versions for this to work reliably. All it did was postpone version compatibility problems until the specific codepaths were executed at runtime &#x2014;this was actually a backward step in terms of fast detection of compatibility problems.</p></div>
<div class="section">
<h3><a name="Missing_method_in_a_Jackson_class"></a>Missing method in a Jackson class</h3>
<p>This is usually caused by version mismatches between Jackson JARs on the classpath. All Jackson JARs on the classpath <i>must</i> be of the same version.</p></div>
<div class="section">
<h3><a name="Authentication_failure"></a>Authentication failure</h3>
<p>If Hadoop cannot authenticate with the S3 service endpoint, the client retries a number of times before eventually failing. When it finally gives up, it will report a message about signature mismatch:</p>

<div class="source">
<div class="source">
<pre>com.amazonaws.services.s3.model.AmazonS3Exception:
 The request signature we calculated does not match the signature you provided.
 Check your key and signing method.
  (Service: Amazon S3; Status Code: 403; Error Code: SignatureDoesNotMatch,
</pre></div></div>
<p>The likely cause is that you either have the wrong credentials or somehow the credentials were not readable on the host attempting to read or write the S3 Bucket.</p>
<p>Enabling debug logging for the package <tt>org.apache.hadoop.fs.s3a</tt> can help provide more information.</p>
<p>The most common cause is that you have the wrong credentials for any of the current authentication mechanism(s) &#x2014;or somehow the credentials were not readable on the host attempting to read or write the S3 Bucket. However, there are a couple of system configuration problems (JVM version, system clock) which also need to be checked.</p>
<p>Most common: there&#x2019;s an error in the configuration properties.</p>

<ol style="list-style-type: decimal">
  
<li>
<p>Make sure that the name of the bucket is the correct one. That is: check the URL.</p></li>
  
<li>
<p>Make sure the property names are correct. For S3A, they are <tt>fs.s3a.access.key</tt> and <tt>fs.s3a.secret.key</tt> &#x2014;you cannot just copy the S3N properties and replace <tt>s3n</tt> with <tt>s3a</tt>.</p></li>
  
<li>
<p>Make sure the properties are visible to the process attempting to talk to the object store. Placing them in <tt>core-site.xml</tt> is the standard mechanism.</p></li>
  
<li>
<p>If using session authentication, the session may have expired. Generate a new session token and secret.</p></li>
  
<li>
<p>If using environement variable-based authentication, make sure that the relevant variables are set in the environment in which the process is running.</p></li>
</ol>
<p>The standard first step is: try to use the AWS command line tools with the same credentials, through a command such as:</p>

<div class="source">
<div class="source">
<pre>hdfs fs -ls s3a://my-bucket/
</pre></div></div>
<p>Note the trailing &#x201c;/&#x201d; here; without that the shell thinks you are trying to list your home directory under the bucket, which will only exist if explicitly created.</p>
<p>Attempting to list a bucket using inline credentials is a means of verifying that the key and secret can access a bucket;</p>

<div class="source">
<div class="source">
<pre>hdfs fs -ls s3a://key:secret@my-bucket/
</pre></div></div>
<p>Do escape any <tt>+</tt> or <tt>/</tt> symbols in the secret, as discussed below, and never share the URL, logs generated using it, or use such an inline authentication mechanism in production.</p>
<p>Finally, if you set the environment variables, you can take advantage of S3A&#x2019;s support of environment-variable authentication by attempting the same ls operation. That is: unset the <tt>fs.s3a</tt> secrets and rely on the environment variables.</p>
<div class="section">
<h4><a name="Authentication_failure_due_to_clock_skew"></a>Authentication failure due to clock skew</h4>
<p>The timestamp is used in signing to S3, so as to defend against replay attacks. If the system clock is too far behind <i>or ahead</i> of Amazon&#x2019;s, requests will be rejected.</p>
<p>This can surface as the situation where read requests are allowed, but operations which write to the bucket are denied.</p>
<p>Check the system clock.</p></div>
<div class="section">
<h4><a name="Authentication_failure_when_using_URLs_with_embedded_secrets"></a>Authentication failure when using URLs with embedded secrets</h4>
<p>If using the (strongly discouraged) mechanism of including the AWS Key and secret in a URL, then both &#x201c;+&#x201d; and &#x201c;/&#x201d; symbols need to encoded in the URL. As many AWS secrets include these characters, encoding problems are not uncommon.</p>

<table border="0" class="bodyTable">
  <thead>
    
<tr class="a">
      
<th>symbol </th>
      
<th>encoded value</th>
    </tr>
  </thead>
  <tbody>
    
<tr class="b">
      
<td><tt>+</tt> </td>
      
<td><tt>%2B</tt> </td>
    </tr>
    
<tr class="a">
      
<td><tt>/</tt> </td>
      
<td><tt>%2F</tt> </td>
    </tr>
  </tbody>
</table>
<p>As an example, a URL for <tt>bucket</tt> with AWS ID <tt>user1</tt> and secret <tt>a+b/c</tt> would be represented as</p>

<div class="source">
<div class="source">
<pre>s3a://user1:a%2Bb%2Fc@bucket/
</pre></div></div>
<p>This technique is only needed when placing secrets in the URL. Again, this is something users are strongly advised against using.</p></div>
<div class="section">
<h4><a name="Authentication_Failures_When_Running_on_Java_8u60"></a>Authentication Failures When Running on Java 8u60+</h4>
<p>A change in the Java 8 JVM broke some of the <tt>toString()</tt> string generation of Joda Time 2.8.0, which stopped the Amazon S3 client from being able to generate authentication headers suitable for validation by S3.</p>
<p><b>Fix</b>: Make sure that the version of Joda Time is 2.8.1 or later, or use a new version of Java 8.</p></div></div>
<div class="section">
<h3><a name="aBad_Request_exception_when_working_with_AWS_S3_Frankfurt_Seoul_or_other_V4_endpoint"></a>&#x201c;Bad Request&#x201d; exception when working with AWS S3 Frankfurt, Seoul, or other &#x201c;V4&#x201d; endpoint</h3>
<p>S3 Frankfurt and Seoul <i>only</i> support <a class="externalLink" href="http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html">the V4 authentication API</a>.</p>
<p>Requests using the V2 API will be rejected with 400 <tt>Bad Request</tt></p>

<div class="source">
<div class="source">
<pre>$ bin/hadoop fs -ls s3a://frankfurt/
WARN s3a.S3AFileSystem: Client: Amazon S3 error 400: 400 Bad Request; Bad Request (retryable)

com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 923C5D9E75E44C06), S3 Extended Request ID: HDwje6k+ANEeDsM6aJ8+D5gUmNAMguOk2BvZ8PH3g9z0gpH+IuwT7N19oQOnIr5CIx7Vqb/uThE=
    at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)
    at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1107)
    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1070)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:307)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:284)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2793)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:101)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2830)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2812)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
    at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
    at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:235)
    at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:218)
    at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)
    at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
    at org.apache.hadoop.fs.FsShell.run(FsShell.java:315)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
    at org.apache.hadoop.fs.FsShell.main(FsShell.java:373)
ls: doesBucketExist on frankfurt-new: com.amazonaws.services.s3.model.AmazonS3Exception:
  Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request;
</pre></div></div>
<p>This happens when trying to work with any S3 service which only supports the &#x201c;V4&#x201d; signing API &#x2014;but the client is configured to use the default S3A service endpoint.</p>
<p>The S3A client needs to be given the endpoint to use via the <tt>fs.s3a.endpoint</tt> property.</p>
<p>As an example, the endpoint for S3 Frankfurt is <tt>s3.eu-central-1.amazonaws.com</tt>:</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;
  &lt;value&gt;s3.eu-central-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="Error_message_The_bucket_you_are_attempting_to_access_must_be_addressed_using_the_specified_endpoint"></a>Error message &#x201c;The bucket you are attempting to access must be addressed using the specified endpoint&#x201d;</h3>
<p>This surfaces when <tt>fs.s3a.endpoint</tt> is configured to use an S3 service endpoint which is neither the original AWS one, <tt>s3.amazonaws.com</tt> , nor the one where the bucket is hosted. The error message contains the redirect target returned by S3, which can be used to determine the correct value for <tt>fs.s3a.endpoint</tt>.</p>

<div class="source">
<div class="source">
<pre>org.apache.hadoop.fs.s3a.AWSS3IOException: Received permanent redirect response
  to bucket.s3-us-west-2.amazonaws.com.  This likely indicates that the S3
  endpoint configured in fs.s3a.endpoint does not match the AWS region
  containing the bucket.: The bucket you are attempting to access must be
  addressed using the specified endpoint. Please send all future requests to
  this endpoint. (Service: Amazon S3; Status Code: 301;
  Error Code: PermanentRedirect; Request ID: 7D39EC1021C61B11)
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:132)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initMultipartUploads(S3AFileSystem.java:287)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:203)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2895)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:102)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2932)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2914)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:390)
</pre></div></div>

<ol style="list-style-type: decimal">
  
<li>Use the <a class="externalLink" href="http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region">Specific endpoint of the bucket&#x2019;s S3 service</a></li>
  
<li>If not using &#x201c;V4&#x201d; authentication (see above), the original S3 endpoint can be used:</li>
</ol>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;
  &lt;value&gt;s3.amazonaws.com&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
<p>Using the explicit endpoint for the region is recommended for speed and to use the V4 signing API.</p></div>
<div class="section">
<h3><a name="aTimeout_waiting_for_connection_from_pool_when_writing_to_S3A"></a>&#x201c;Timeout waiting for connection from pool&#x201d; when writing to S3A</h3>
<p>This happens when using the Block output stream, <tt>fs.s3a.fast.upload=true</tt> and the thread pool runs out of capacity.</p>

<div class="source">
<div class="source">
<pre>[s3a-transfer-shared-pool1-t20] INFO  http.AmazonHttpClient (AmazonHttpClient.java:executeHelper(496)) - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool
  at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:230)
  at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:199)
  at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)
  at com.amazonaws.http.conn.$Proxy10.getConnection(Unknown Source)
  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:424)
  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)
  at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:728)
  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
  at com.amazonaws.services.s3.AmazonS3Client.doUploadPart(AmazonS3Client.java:2921)
  at com.amazonaws.services.s3.AmazonS3Client.uploadPart(AmazonS3Client.java:2906)
  at org.apache.hadoop.fs.s3a.S3AFileSystem.uploadPart(S3AFileSystem.java:1025)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload$1.call(S3ABlockOutputStream.java:360)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload$1.call(S3ABlockOutputStream.java:355)
  at org.apache.hadoop.fs.s3a.BlockingThreadPoolExecutorService$CallableWithPermitRelease.call(BlockingThreadPoolExecutorService.java:239)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
</pre></div></div>
<p>Make sure that <tt>fs.s3a.connection.maximum</tt> is at least larger than <tt>fs.s3a.threads.max</tt>.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.max&lt;/name&gt;
  &lt;value&gt;20&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.maximum&lt;/name&gt;
  &lt;value&gt;30&lt;/value&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="aTimeout_waiting_for_connection_from_pool_when_reading_from_S3A"></a>&#x201c;Timeout waiting for connection from pool&#x201d; when reading from S3A</h3>
<p>This happens when more threads are trying to read from an S3A system than the maximum number of allocated HTTP connections.</p>
<p>Set <tt>fs.s3a.connection.maximum</tt> to a larger value (and at least as large as <tt>fs.s3a.threads.max</tt>)</p></div>
<div class="section">
<h3><a name="Out_of_heap_memory_when_writing_to_S3A_via_Fast_Upload"></a>Out of heap memory when writing to S3A via Fast Upload</h3>
<p>This can happen when using the fast upload mechanism (<tt>fs.s3a.fast.upload=true</tt>) and in-memory buffering (either <tt>fs.s3a.fast.upload.buffer=array</tt> or <tt>fs.s3a.fast.upload.buffer=bytebuffer</tt>).</p>
<p>More data is being generated than in the JVM than it can upload to S3 &#x2014;and so much data has been buffered that the JVM has run out of memory.</p>
<p>Consult <a href="#s3a_fast_upload_thread_tuning">S3A Fast Upload Thread Tuning</a> for detail on this issue and options to address it. Consider also buffering to disk, rather than memory.</p></div>
<div class="section">
<h3><a name="When_writing_to_S3A:_java.io.FileNotFoundException:_Completing_multi-part_upload"></a>When writing to S3A: &#x201c;java.io.FileNotFoundException: Completing multi-part upload&#x201d;</h3>

<div class="source">
<div class="source">
<pre>java.io.FileNotFoundException: Completing multi-part upload on fork-5/test/multipart/1c397ca6-9dfb-4ac1-9cf7-db666673246b: com.amazonaws.services.s3.model.AmazonS3Exception: The specified upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchUpload; Request ID: 84FF8057174D9369), S3 Extended Request ID: Ij5Yn6Eq/qIERH4Z6Io3YL2t9/qNZ7z9gjPb1FrTtTovZ8k1MXqh+zCYYjqmfJ/fCY6E1+JR9jA=
  at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)
  at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)
  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
  at com.amazonaws.services.s3.AmazonS3Client.completeMultipartUpload(AmazonS3Client.java:2705)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:473)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$200(S3ABlockOutputStream.java:382)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:272)
  at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
  at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
</pre></div></div>
<p>This surfaces if, while a multipart upload was taking place, all outstanding multipart uploads were garbage collected. The upload operation cannot complete because the data uploaded has been deleted.</p>
<p>Consult <a href="#s3a_multipart_purge">Cleaning up After Incremental Upload Failures</a> for details on how the multipart purge timeout can be set. If multipart uploads are failing with the message above, it may be a sign that this value is too low.</p></div>
<div class="section">
<h3><a name="MultiObjectDeleteException_during_delete_or_rename_of_files"></a><tt>MultiObjectDeleteException</tt> during delete or rename of files</h3>

<div class="source">
<div class="source">
<pre>Exception in thread &quot;main&quot; com.amazonaws.services.s3.model.MultiObjectDeleteException:
    Status Code: 0, AWS Service: null, AWS Request ID: null, AWS Error Code: null,
    AWS Error Message: One or more objects could not be deleted, S3 Extended Request ID: null
  at com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:1745)
</pre></div></div>
<p>This happens when trying to delete multiple objects, and one of the objects could not be deleted. It <i>should not occur</i> just because the object is missing. More specifically: at the time this document was written, we could not create such a failure.</p>
<p>It will occur if the caller lacks the permission to delete any of the objects.</p>
<p>Consult the log to see the specifics of which objects could not be deleted. Do you have permission to do so?</p>
<p>If this operation is failing for reasons other than the caller lacking permissions:</p>

<ol style="list-style-type: decimal">
  
<li>Try setting <tt>fs.s3a.multiobjectdelete.enable</tt> to <tt>false</tt>.</li>
  
<li>Consult <a class="externalLink" href="https://issues.apache.org/jira/browse/HADOOP-11572">HADOOP-11572</a> for up to date advice.</li>
</ol></div>
<div class="section">
<h3><a name="When_writing_to_S3A_HTTP_Exceptions_logged_at_info_from_AmazonHttpClient"></a>When writing to S3A, HTTP Exceptions logged at info from <tt>AmazonHttpClient</tt></h3>

<div class="source">
<div class="source">
<pre>[s3a-transfer-shared-pool4-t6] INFO  http.AmazonHttpClient (AmazonHttpClient.java:executeHelper(496)) - Unable to execute HTTP request: hwdev-steve-ireland-new.s3.amazonaws.com:443 failed to respond
org.apache.http.NoHttpResponseException: bucket.s3.amazonaws.com:443 failed to respond
  at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:143)
  at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:57)
  at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:261)
  at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:283)
  at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:259)
  at org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:209)
  at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:272)
  at com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:66)
  at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:124)
  at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:686)
  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:488)
  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)
  at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:728)
  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
  at com.amazonaws.services.s3.AmazonS3Client.copyPart(AmazonS3Client.java:1731)
  at com.amazonaws.services.s3.transfer.internal.CopyPartCallable.call(CopyPartCallable.java:41)
  at com.amazonaws.services.s3.transfer.internal.CopyPartCallable.call(CopyPartCallable.java:28)
  at org.apache.hadoop.fs.s3a.BlockingThreadPoolExecutorService$CallableWithPermitRelease.call(BlockingThreadPoolExecutorService.java:239)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
</pre></div></div>
<p>These are HTTP I/O exceptions caught and logged inside the AWS SDK. The client will attempt to retry the operation; it may just be a transient event. If there are many such exceptions in logs, it may be a symptom of connectivity or network problems.</p></div>
<div class="section">
<h3><a name="Visible_S3_Inconsistency"></a>Visible S3 Inconsistency</h3>
<p>Amazon S3 is <i>an eventually consistent object store</i>. That is: not a filesystem.</p>
<p>It offers read-after-create consistency: a newly created file is immediately visible. Except, there is a small quirk: a negative GET may be cached, such that even if an object is immediately created, the fact that there &#x201c;wasn&#x2019;t&#x201d; an object is still remembered.</p>
<p>That means the following sequence on its own will be consistent <tt>
touch(path) -&gt; getFileStatus(path)
</tt></p>
<p>But this sequence <i>may</i> be inconsistent.</p>

<div class="source">
<div class="source">
<pre>getFileStatus(path) -&gt; touch(path) -&gt; getFileStatus(path)
</pre></div></div>
<p>A common source of visible inconsistencies is that the S3 metadata database &#x2014;the part of S3 which serves list requests&#x2014; is updated asynchronously. Newly added or deleted files may not be visible in the index, even though direct operations on the object (<tt>HEAD</tt> and <tt>GET</tt>) succeed.</p>
<p>In S3A, that means the <tt>getFileStatus()</tt> and <tt>open()</tt> operations are more likely to be consistent with the state of the object store than any directory list operations (<tt>listStatus()</tt>, <tt>listFiles()</tt>, <tt>listLocatedStatus()</tt>, <tt>listStatusIterator()</tt>).</p></div>
<div class="section">
<h3><a name="FileNotFoundException_even_though_the_file_was_just_written."></a><tt>FileNotFoundException</tt> even though the file was just written.</h3>
<p>This can be a sign of consistency problems. It may also surface if there is some asynchronous file write operation still in progress in the client: the operation has returned, but the write has not yet completed. While the S3A client code does block during the <tt>close()</tt> operation, we suspect that asynchronous writes may be taking place somewhere in the stack &#x2014;this could explain why parallel tests fail more often than serialized tests.</p></div>
<div class="section">
<h3><a name="File_not_found_in_a_directory_listing_even_though_getFileStatus_finds_it"></a>File not found in a directory listing, even though <tt>getFileStatus()</tt> finds it</h3>
<p>(Similarly: deleted file found in listing, though <tt>getFileStatus()</tt> reports that it is not there)</p>
<p>This is a visible sign of updates to the metadata server lagging behind the state of the underlying filesystem.</p></div>
<div class="section">
<h3><a name="File_not_visiblesaved"></a>File not visible/saved</h3>
<p>The files in an object store are not visible until the write has been completed. In-progress writes are simply saved to a local file/cached in RAM and only uploaded. at the end of a write operation. If a process terminated unexpectedly, or failed to call the <tt>close()</tt> method on an output stream, the pending data will have been lost.</p></div>
<div class="section">
<h3><a name="File_flush_and_hflush_calls_do_not_save_data_to_S3A"></a>File <tt>flush()</tt> and <tt>hflush()</tt> calls do not save data to S3A</h3>
<p>Again, this is due to the fact that the data is cached locally until the <tt>close()</tt> operation. The S3A filesystem cannot be used as a store of data if it is required that the data is persisted durably after every <tt>flush()/hflush()</tt> call. This includes resilient logging, HBase-style journalling and the like. The standard strategy here is to save to HDFS and then copy to S3.</p></div>
<div class="section">
<h3><a name="S3_Server_Side_Encryption"></a>S3 Server Side Encryption</h3>
<div class="section">
<h4><a name="Using_SSE-KMS"></a>Using SSE-KMS</h4>
<p>When performing file operations, the user may run into an issue where the KMS key arn is invalid. <tt>
com.amazonaws.services.s3.model.AmazonS3Exception:
Invalid arn (Service: Amazon S3; Status Code: 400; Error Code: KMS.NotFoundException; Request ID: 708284CF60EE233F),
S3 Extended Request ID: iHUUtXUSiNz4kv3Bdk/hf9F+wjPt8GIVvBHx/HEfCBYkn7W6zmpvbA3XT7Y5nTzcZtfuhcqDunw=:
Invalid arn (Service: Amazon S3; Status Code: 400; Error Code: KMS.NotFoundException; Request ID: 708284CF60EE233F)
</tt></p>
<p>This is due to either, the KMS key id is entered incorrectly, or the KMS key id is in a different region than the S3 bucket being used.</p></div>
<div class="section">
<h4><a name="Using_SSE-C"></a>Using SSE-C</h4>
<p>When performing file operations the user may run into an unexpected 400/403 error such as <tt>
org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on fork-4/: com.amazonaws.services.s3.model.AmazonS3Exception:
Bad Request (Service: Amazon S3; Status Code: 400;
Error Code: 400 Bad Request; Request ID: 42F9A1987CB49A99),
S3 Extended Request ID: jU2kcwaXnWj5APB14Cgb1IKkc449gu2+dhIsW/+7x9J4D+VUkKvu78mBo03oh9jnOT2eoTLdECU=:
Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 42F9A1987CB49A99)
</tt></p>
<p>This can happen in the cases of not specifying the correct SSE-C encryption key. Such cases can be as follows: 1. An object is encrypted using SSE-C on S3 and either the wrong encryption type is used, no encryption is specified, or the SSE-C specified is incorrect. 2. A directory is encrypted with a SSE-C keyA and the user is trying to move a file using configured SSE-C keyB into that structure.</p></div></div>
<div class="section">
<h3><a name="Other_issues"></a>Other issues</h3>
<p><i>Performance slow</i></p>
<p>S3 is slower to read data than HDFS, even on virtual clusters running on Amazon EC2.</p>

<ul>
  
<li>HDFS replicates data for faster query performance</li>
  
<li>HDFS stores the data on the local hard disks, avoiding network traffic  if the code can be executed on that host. As EC2 hosts often have their  network bandwidth throttled, this can make a tangible difference.</li>
  
<li>HDFS is significantly faster for many &#x201c;metadata&#x201d; operations: listing the contents of a directory, calling <tt>getFileStatus()</tt> on path, creating or deleting directories.</li>
  
<li>On HDFS, Directory renames and deletes are <tt>O(1)</tt> operations. On S3 renaming is a very expensive <tt>O(data)</tt> operation which may fail partway through in which case the final state depends on where the copy+ delete sequence was when it failed. All the objects are copied, then the original set of objects are deleted, so a failure should not lose data &#x2014;it may result in duplicate datasets.</li>
  
<li>Because the write only begins on a <tt>close()</tt> operation, it may be in the final phase of a process where the write starts &#x2014;this can take so long that some things can actually time out.</li>
  
<li>File IO performing many seek calls/positioned read calls will encounter performance problems due to the size of the HTTP requests made. On S3a, the (experimental) fadvise policy &#x201c;random&#x201d; can be set to alleviate this at the expense of sequential read performance and bandwidth.</li>
</ul>
<p>The slow performance of <tt>rename()</tt> surfaces during the commit phase of work, including</p>

<ul>
  
<li>The MapReduce <tt>FileOutputCommitter</tt>.</li>
  
<li>DistCp&#x2019;s rename-after-copy operation.</li>
  
<li>The <tt>hdfs fs -rm</tt> command renaming the file under <tt>.Trash</tt> rather than deleting it. Use <tt>-skipTrash</tt> to eliminate that step.</li>
</ul>
<p>These operations can be significantly slower when S3 is the destination compared to HDFS or other &#x201c;real&#x201d; filesystem.</p>
<p><i>Improving S3 load-balancing behavior</i></p>
<p>Amazon S3 uses a set of front-end servers to provide access to the underlying data. The choice of which front-end server to use is handled via load-balancing DNS service: when the IP address of an S3 bucket is looked up, the choice of which IP address to return to the client is made based on the the current load of the front-end servers.</p>
<p>Over time, the load across the front-end changes, so those servers considered &#x201c;lightly loaded&#x201d; will change. If the DNS value is cached for any length of time, your application may end up talking to an overloaded server. Or, in the case of failures, trying to talk to a server that is no longer there.</p>
<p>And by default, for historical security reasons in the era of applets, the DNS TTL of a JVM is &#x201c;infinity&#x201d;.</p>
<p>To work with AWS better, set the DNS time-to-live of an application which works with S3 to something lower. See <a class="externalLink" href="http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/java-dg-jvm-ttl.html">AWS documentation</a>.</p></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">
        &#169;            2018
              Apache Software Foundation
            
                          - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a>.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
